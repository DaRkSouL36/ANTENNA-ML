{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ea3778",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223115b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n",
      "OS: Windows-11-10.0.22631-SP0\n",
      "NUMPY: 2.1.3\n",
      "PANDAS: 2.3.1\n",
      "SCIKIT-LEARN: 1.7.1\n",
      "MATPLOTLIB: 3.10.3\n",
      "SEABORN: 0.13.2\n",
      "PLOTLY: 6.2.0\n",
      "OPTUNA: 4.4.0\n",
      "SHAP: 0.48.0\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND PRINTS ALL VERSIONS SO WE CAN REPRODUCE RESULTS EXACTLY LATER              \n",
    "\n",
    "import sys                                           # STANDARD LIB TO ACCESS PYTHON RUNTIME DETAILS\n",
    "import platform                                      # STANDARD LIB TO GET OS/PLATFORM INFORMATION\n",
    "import warnings                                      # STANDARD LIB TO CONTROL WARNING MESSAGES\n",
    "import os                                            # STANDARD LIB TO QUERY CPU COUNT FOR PARALLELISM\n",
    "from pathlib import Path                             # STANDARD LIB FOR SAFE, CROSS-PLATFORM PATH HANDLING\n",
    "from typing import Dict, Any, Tuple                  # TYPE HINTS FOR CLARITY\n",
    "\n",
    "import numpy as np                                   # NUMERICAL COMPUTING\n",
    "import pandas as pd                                  # DATAFRAMES AND DATA MANIPULATION\n",
    "\n",
    "import matplotlib                                    # BASE PLOTTING BACKEND\n",
    "import matplotlib.pyplot as plt                      # STATEFUL PLOTTING INTERFACE\n",
    "from matplotlib import rcParams                      # IMPORTS RCPARAMS TO SET GLOBAL STYLES\n",
    "import seaborn as sns                                # STATISTICAL PLOTTING BUILT ON TOP OF MATPLOTLIB\n",
    "import plotly                                        # INTERACTIVE PLOTTING (NOT USED HERE BUT KEPT FOR CONSISTENCY)\n",
    "\n",
    "import sklearn                                       # SCIKIT-LEARN: CLASSIC MACHINE LEARNING\n",
    "from sklearn.model_selection import train_test_split # TRAIN/TEST SPLITTING\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate  # K-FOLD SPLITTING AND CV\n",
    "from sklearn.pipeline import Pipeline                # TO BUILD CLEAN, REUSABLE PREPROCESSING PIPELINES\n",
    "from sklearn.impute import SimpleImputer             # SIMPLE STRATEGIES TO IMPUTE MISSING VALUES\n",
    "from sklearn.preprocessing import (                  # SCALERS + TRANSFORMS\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    PolynomialFeatures, PowerTransformer\n",
    ")  \n",
    "from sklearn.compose import ColumnTransformer        # APPLY TRANSFORMS TO COLUMNS (WE USE ALL NUMERIC)\n",
    "from sklearn.metrics import (                        # METRICS FOR REGRESSION\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    explained_variance_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor    # WRAPS SINGLE-OUTPUT MODELS FOR MULTI-OUTPUT TARGETS\n",
    "from sklearn.linear_model import (                      # LINEAR FAMILY\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet\n",
    ")  \n",
    "from sklearn.neighbors import KNeighborsRegressor       # KNN REGRESSOR\n",
    "from sklearn.svm import SVR                             # SUPPORT VECTOR REGRESSION\n",
    "from sklearn.preprocessing import PolynomialFeatures    # POLYNOMIAL FEATURES\n",
    "from sklearn.compose import TransformedTargetRegressor  # FOR SCALING TARGETS\n",
    "from sklearn.ensemble import StackingRegressor          # FOR STACKING REGRESSOR\n",
    "\n",
    "\n",
    "import optuna                                        # HYPERPARAMETER OPTIMIZATION\n",
    "from optuna.samplers import TPESampler               # ADVANCED SAMPLER\n",
    "from optuna.pruners import MedianPruner              # EARLY STOPPING PRUNER\n",
    "import shap                                          # MODEL INTERPRETABILITY \n",
    "import joblib                                        # SERIALIZATION (SAVING/LOADING MODELS)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")                    # SUPPRESSES NON-CRITICAL WARNINGS FOR CLEANER OUTPUT\n",
    "\n",
    "print(\"PYTHON:\", sys.version)                        # PRINTS PYTHON VERSION\n",
    "print(\"OS:\", platform.platform())                    # PRINTS OPERATING SYSTEM DETAILS\n",
    "print(\"NUMPY:\", np.__version__)                      # PRINTS NUMPY VERSION\n",
    "print(\"PANDAS:\", pd.__version__)                     # PRINTS PANDAS VERSION\n",
    "print(\"SCIKIT-LEARN:\", sklearn.__version__)          # PRINTS SCIKIT-LEARN VERSION\n",
    "print(\"MATPLOTLIB:\", matplotlib.__version__)         # PRINTS MATPLOTLIB VERSION\n",
    "print(\"SEABORN:\", sns.__version__)                   # PRINTS SEABORN VERSION\n",
    "print(\"PLOTLY:\", plotly.__version__)                 # PRINTS PLOTLY VERSION\n",
    "print(\"OPTUNA:\", optuna.__version__)                 # PRINTS OPTUNA VERSION\n",
    "print(\"SHAP:\", shap.__version__)                     # PRINTS SHAP VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d951d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\antenna-ml-thz\\venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "SCIKIT-LEARN: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -U scikit-learn\n",
    "print(\"SCIKIT-LEARN:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dc27c",
   "metadata": {},
   "source": [
    "### PLOTTING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708af0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PLOTTING STYLE TO KEEP ALL FIGURES CRISP, BOLD, UPPERCASE, AND HIGH-IMPACT      \n",
    "FIGSIZE = (8, 6)                                      # DEFAULT FIGURE SIZE\n",
    "DPI = 500                                             # HIGH RESOLUTION\n",
    "LINEWIDTH = 2.0                                       # BOLD LINE WIDTH\n",
    "GRID_LINEWIDTH = 1.5                                  # BOLD GRID LINES\n",
    "FONTSIZE_TITLE = 20                                   # LARGE TITLE SIZE\n",
    "FONTSIZE_LABEL = 16                                   # LARGE AXIS LABEL SIZE\n",
    "FONTSIZE_TICK = 14                                    # LARGE TICK LABEL SIZE\n",
    "FONTSIZE_LEGEND = 14                                  # LARGE LEGEND FONT SIZE\n",
    "\n",
    "def init_plot_style() -> None:                        # DEFINES A FUNCTION TO INITIALIZE GLOBAL STYLE\n",
    "    \"\"\"SET GLOBAL MATPLOTLIB STYLE FOR BOLD, UPPERCASE, HIGH-RES FIGURES.\"\"\"  \n",
    "    rcParams[\"figure.figsize\"] = FIGSIZE              # SETS FIGURE SIZE\n",
    "    rcParams[\"figure.dpi\"] = DPI                      # SETS DPI\n",
    "    rcParams[\"savefig.dpi\"] = DPI                     # HIGH-RES SAVED FIGURES\n",
    "    rcParams[\"font.weight\"] = \"bold\"                  # MAKES TEXT BOLD\n",
    "    rcParams[\"axes.titleweight\"] = \"bold\"             # BOLD TITLES\n",
    "    rcParams[\"axes.labelweight\"] = \"bold\"             # BOLD LABELS\n",
    "    rcParams[\"axes.titlesize\"] = FONTSIZE_TITLE       # TITLE FONT SIZE\n",
    "    rcParams[\"axes.labelsize\"] = FONTSIZE_LABEL       # LABEL FONT SIZE\n",
    "    rcParams[\"xtick.labelsize\"] = FONTSIZE_TICK       # X-TICK LABEL SIZE\n",
    "    rcParams[\"ytick.labelsize\"] = FONTSIZE_TICK       # Y-TICK LABEL SIZE\n",
    "    rcParams[\"legend.fontsize\"] = FONTSIZE_LEGEND     # LEGEND FONT SIZE\n",
    "    rcParams[\"legend.title_fontsize\"] = FONTSIZE_LEGEND  # LEGEND TITLE FONT SIZE\n",
    "    rcParams[\"lines.linewidth\"] = LINEWIDTH           # DEFAULT LINE WIDTH\n",
    "    rcParams[\"grid.linewidth\"] = GRID_LINEWIDTH       # GRID LINE WIDTH\n",
    "    rcParams[\"axes.grid\"] = True                      # ENABLE GRID BY DEFAULT\n",
    "    rcParams[\"grid.alpha\"] = 0.3                      # GRID TRANSPARENCY\n",
    "    rcParams[\"axes.spines.top\"] = True                # SHOW TOP SPINE\n",
    "    rcParams[\"axes.spines.right\"] = True              # SHOW RIGHT SPINE\n",
    "\n",
    "def boldify_axes(ax: plt.Axes,\n",
    "                 title: str = \"\",\n",
    "                 xlabel: str = \"\",\n",
    "                 ylabel: str = \"\",\n",
    "                 legend: bool = True) -> None:\n",
    "    \"\"\"UPPERCASE + BOLD ALL TEXT ELEMENTS ON AN AXES OBJECT.\"\"\"  \n",
    "    if title:                                                    # CHECKS IF TITLE IS PROVIDED\n",
    "        ax.set_title(title.upper(), weight=\"bold\", size=FONTSIZE_TITLE)    # SETS BOLD, UPPERCASE TITLE\n",
    "    if xlabel:                                                   # CHECKS IF XLABEL IS PROVIDED\n",
    "        ax.set_xlabel(xlabel.upper(), weight=\"bold\", size=FONTSIZE_LABEL)  # SETS BOLD, UPPERCASE XLABEL\n",
    "    if ylabel:                                                   # CHECKS IF YLABEL IS PROVIDED\n",
    "        ax.set_ylabel(ylabel.upper(), weight=\"bold\", size=FONTSIZE_LABEL)  # SETS BOLD, UPPERCASE YLABEL\n",
    "\n",
    "    for tick in ax.get_xticklabels():                             # LOOPS OVER X TICKS\n",
    "        tick.set_fontweight(\"bold\")                               # MAKES THEM BOLD\n",
    "        tick.set_fontsize(FONTSIZE_TICK)                          # SETS FONT SIZE\n",
    "        tick.set_text(str(tick.get_text()).upper())               # UPPERCASES TEXT\n",
    "\n",
    "    for tick in ax.get_yticklabels():                             # LOOPS OVER Y TICKS\n",
    "        tick.set_fontweight(\"bold\")                               # MAKES THEM BOLD\n",
    "        tick.set_fontsize(FONTSIZE_TICK)                          # SETS FONT SIZE\n",
    "        tick.set_text(str(tick.get_text()).upper())               # UPPERCASES TEXT\n",
    "\n",
    "    for spine in ax.spines.values():                              # ITERATES OVER SPINES\n",
    "        spine.set_linewidth(2.0)                                  # MAKES SPINES THICK\n",
    "\n",
    "    if legend and ax.get_legend() is not None:                     # IF LEGEND EXISTS, FORMAT IT\n",
    "        leg = ax.get_legend()                                      # GETS LEGEND HANDLE\n",
    "        if leg.get_title() is not None:                            # IF LEGEND TITLE EXISTS\n",
    "            leg.get_title().set_text(leg.get_title().get_text().upper())  # UPPERCASE LEGEND TITLE\n",
    "            leg.get_title().set_fontweight(\"bold\")                 # BOLD LEGEND TITLE\n",
    "        for text in leg.get_texts():                               # FOR EACH LEGEND LABEL\n",
    "            text.set_text(text.get_text().upper())                 # UPPERCASE TEXT\n",
    "            text.set_fontweight(\"bold\")                            # BOLD TEXT\n",
    "            text.set_fontsize(FONTSIZE_LEGEND)                     # SET FONT SIZE\n",
    "\n",
    "def finalize_figure(fig: plt.Figure, suptitle: str = \"\") -> None:\n",
    "    \"\"\"APPLY SUPTITLE (UPPERCASE, BOLD) AND TIGHT LAYOUT.\"\"\"       \n",
    "    if suptitle:                                                   # IF SUPTITLE PROVIDED\n",
    "        fig.suptitle(suptitle.upper(), fontsize=FONTSIZE_TITLE, fontweight=\"bold\")  # SETS BOLD, UPPERCASE SUPTITLE\n",
    "    fig.tight_layout()                                             # TIGHT LAYOUT TO PREVENT CLIPPING\n",
    "\n",
    "init_plot_style()                                                  # INITIALIZES THE GLOBAL STYLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cecb1",
   "metadata": {},
   "source": [
    "### CONFIG & REPRODUCIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2e00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL SEED: 42\n",
      "TRAIN-TEST SPLIT: 0.8 0.2\n",
      "K-FOLDS: 5\n",
      "N_TRIALS: 1000 | N_JOBS: 5\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION FOR REPRODUCIBILITY AND PROJECT-SPECIFIC CONSTANTS                       \n",
    "\n",
    "GLOBAL_SEED = 42                                     # GLOBAL SEED FOR REPRODUCIBILITY\n",
    "np.random.seed(GLOBAL_SEED)                          # SETS NUMPY SEED\n",
    "\n",
    "# PATHS (ADAPT LOCALLY): THE USER REQUESTED ..\\DATA\\DATA[P].csv                          \n",
    "DATA_CSV_PATH = Path(\"../DATA/DATA[P].csv\")          # RELATIVE PATH AS SPECIFIED BY USER\n",
    "RESULTS_DIR = Path(\"../DATA/\")                        # DIRECTORY TO SAVE RESULTS\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)       # CREATES DIRECTORY IF NOT EXISTS\n",
    "\n",
    "FEATURE_COLS = [\"il\", \"iw\", \"pw\", \"ro\"]              # FEATURES FROM USER\n",
    "TARGET_COLS = [\"frequency\", \"return loss\", \"gain\"]   # MULTI-OUTPUT TARGETS FROM USER\n",
    "\n",
    "TEST_SIZE = 0.2                                       # 80/20 TRAIN-TEST SPLIT\n",
    "N_SPLITS = 5                                          # 5-FOLD CV\n",
    "OPTUNA_TRIALS = 100                                   # OPTUNA TRIAL BUDGET\n",
    "N_TRIALS = 1000                                       # VERY HIGH-BUDGET SEARCH \n",
    "N_JOBS = max(1, (os.cpu_count() or 2) - 1)            # PARALLEL TRIALS USING AVAILABLE CORES MINUS ONE\n",
    "USE_TARGET_SCALING = True                             # SCALE TARGETS\n",
    "USE_POWER_TRANSFORM = True                            # ALLOW YEO-JOHNSON AS A TUNABLE OPTION\n",
    "USE_POLYNOMIALS = True                                # ALLOW POLYNOMIAL FEATURES AS A TUNABLE OPTION\n",
    "POLY_DEGREE = 2                                       # DEGREE 2 POLYNOMIALS \n",
    "MAX_POLY_DEGREE = 5                                   # UPPER BOUND FOR POLYNOMIAL DEGREE\n",
    "\n",
    "print(\"GLOBAL SEED:\", GLOBAL_SEED)                   # CONFIRMS GLOBAL SEED\n",
    "print(\"TRAIN-TEST SPLIT:\", 1 - TEST_SIZE, TEST_SIZE) # PRINTS TRAIN/TEST RATIO\n",
    "print(\"K-FOLDS:\", N_SPLITS)                          # PRINTS NUMBER OF FOLDS\n",
    "print(\"N_TRIALS:\", N_TRIALS, \"| N_JOBS:\", N_JOBS)    # PRINTS TRIALS AND PARALLEL JOBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f79651",
   "metadata": {},
   "source": [
    "### DATA READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0c5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADED: (1296, 7)\n"
     ]
    }
   ],
   "source": [
    "# SAFE CSV READER TO LOAD THE CLEANED DATA                                               \n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:       # DEFINES A SAFE CSV READER\n",
    "    \"\"\"SAFELY READ A CSV FILE AND RETURN A PANDAS DATAFRAME WITH CLEAR ERRORS.\"\"\"  \n",
    "    if not path.exists():                            # CHECKS IF FILE EXISTS\n",
    "        raise FileNotFoundError(f\"FILE NOT FOUND: {path}\")  # RAISES ERROR IF NOT FOUND\n",
    "    df_local = pd.read_csv(path)                     # READS CSV\n",
    "    if df_local.empty:                               # CHECKS IF EMPTY\n",
    "        raise ValueError(\"THE CSV FILE IS EMPTY.\")   # RAISES ERROR IF EMPTY\n",
    "    return df_local                                  # RETURNS DATAFRAME\n",
    "\n",
    "df = safe_read_csv(DATA_CSV_PATH)                    # LOADS THE CLEANED DATA\n",
    "print(\"DATA LOADED:\", df.shape)                      # PRINTS SHAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc38b6",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT & K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66be0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE (X, Y): (1036, 4) (1036, 3)\n",
      "TEST SHAPE  (X, Y): (260, 4) (260, 3)\n",
      "K-FOLD READY.\n"
     ]
    }
   ],
   "source": [
    "# WE SPLIT ONCE INTO TRAIN/TEST (80/20), THEN USE K-FOLD ON THE TRAIN SET FOR CV        \n",
    "\n",
    "X = df[FEATURE_COLS]                          # EXTRACTS FEATURES AS NUMPY ARRAY\n",
    "Y = df[TARGET_COLS]                           # EXTRACTS TARGETS AS NUMPY ARRAY\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( # PERFORMS TRAIN-TEST SPLIT\n",
    "    X, Y, test_size=TEST_SIZE, random_state=GLOBAL_SEED, shuffle=True\n",
    ")                                                    \n",
    "\n",
    "print(\"TRAIN SHAPE (X, Y):\", X_train.shape, y_train.shape)  # PRINTS TRAIN SHAPES\n",
    "print(\"TEST SHAPE  (X, Y):\", X_test.shape, y_test.shape)    # PRINTS TEST SHAPES\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=GLOBAL_SEED)  # DEFINES K-FOLD SPLITTER\n",
    "print(\"K-FOLD READY.\")                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226c373",
   "metadata": {},
   "source": [
    "### METRICS HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904e8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY TO CALCULATE ALL REQUESTED METRICS IN ONE PLACE                                  \n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]: # DEFINES METRICS FUNCTION\n",
    "    \"\"\"RETURN A DICTIONARY OF REGRESSION METRICS FOR MULTI-OUTPUT TARGETS.\"\"\"       \n",
    "    r2 = r2_score(y_true, y_pred, multioutput=\"uniform_average\")                    # COMPUTES R2\n",
    "    mae = mean_absolute_error(y_true, y_pred, multioutput=\"uniform_average\")        # COMPUTES MAE\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput=\"uniform_average\")         # COMPUTES MSE\n",
    "    rmse = np.sqrt(mse)                                                             # COMPUTES RMSE\n",
    "    evs = explained_variance_score(y_true, y_pred, multioutput=\"uniform_average\")   # COMPUTES EXPLAINED VARIANCE\n",
    "    return {\"r2\": r2, \"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"explained_variance\": evs}  # RETURNS ALL METRICS\n",
    "\n",
    "def print_metrics(name: str, metrics: Dict[str, float]) -> None:      # DEFINES PRINTING FUNCTION\n",
    "    \"\"\"PRETTY-PRINT METRICS WITH UPPERCASE KEYS.\"\"\"                    \n",
    "    print(f\"=== {name.upper()} METRICS ===\")                          # PRINTS HEADER\n",
    "    for k, v in metrics.items():                                      # LOOPS OVER METRICS\n",
    "        print(f\"{k.upper()}: {v:.4f}\")                                # PRINTS EACH METRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b25ab",
   "metadata": {},
   "source": [
    "### PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a575b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSORS READY. TARGET SCALING: True\n"
     ]
    }
   ],
   "source": [
    "# NUMERIC PIPELINE FOR FEATURES: IMPUTE MEDIAN + STANDARD SCALER                         \n",
    "\n",
    "numeric_transformer = Pipeline(steps=[               # BUILDS A PIPELINE\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # MISSING VALUE IMPUTATION\n",
    "    (\"scaler\", StandardScaler()),                    # STANDARD SCALING\n",
    "])                                                   # CLOSES PIPELINE\n",
    "\n",
    "preprocessor = ColumnTransformer(                    # WRAPS TRANSFORMER (ALL COLUMNS ARE NUMERIC)\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],  # APPLIES TO ALL FEATURE COLS\n",
    "    remainder=\"drop\"                                 # DROPS ANY OTHER COLUMNS (THERE ARE NONE)\n",
    ")                                                    \n",
    "\n",
    "y_scaler = StandardScaler() if USE_TARGET_SCALING else None  # CREATES Y SCALER IF REQUESTED\n",
    "\n",
    "print(\"PREPROCESSORS READY. TARGET SCALING:\", USE_TARGET_SCALING)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa03cb1",
   "metadata": {},
   "source": [
    "### CROSS-VALIDATION EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a6336bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER TO RUN K-FOLD CV AND RETURN MEAN/STD R2, PLUS FIT ON FULL TRAIN AND EVAL TEST     \n",
    "\n",
    "def evaluate_sklearn_pipeline(name: str, pipe: Pipeline) -> Dict[str, Any]:  # DEFINES EVALUATION FUNCTION\n",
    "    \"\"\"FIT/VALIDATE A SKLEARN PIPELINE WITH K-FOLD AND TEST EVALUATION.\"\"\"   \n",
    "    # CROSS-VALIDATION R2 SCORES                                                     \n",
    "    cv_scores = cross_val_score(pipe, X_train, y_train, cv=kf, scoring=\"r2\", n_jobs=-1)  # RUNS CV\n",
    "    pipe.fit(X_train, y_train)                                                    # FITS ON FULL TRAIN\n",
    "    y_pred_test = pipe.predict(X_test)                                            # PREDICTS ON TEST\n",
    "    metrics = regression_metrics(y_test, y_pred_test)                             # COMPUTES METRICS\n",
    "    result = {                                                                    # BUILDS RESULT DICT\n",
    "        \"model\": name,\n",
    "        \"cv_r2_mean\": np.mean(cv_scores),\n",
    "        \"cv_r2_std\": np.std(cv_scores),\n",
    "        **metrics\n",
    "    }                                                                              # CLOSES DICT\n",
    "    print_metrics(name, metrics)                                                   # PRINTS TEST METRICS\n",
    "    print(f\"CV R2 MEAN: {result['cv_r2_mean']:.4f} | CV R2 STD: {result['cv_r2_std']:.4f}\")  # PRINTS CV SCORES\n",
    "    return result                                                                   # RETURNS RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20b2be",
   "metadata": {},
   "source": [
    "### SVR MODEL WITH BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dadde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEST_SVR METRICS ===\n",
      "R2: 0.8381\n",
      "MAE: 0.9642\n",
      "MSE: 4.4526\n",
      "RMSE: 2.1101\n",
      "EXPLAINED_VARIANCE: 0.8388\n",
      "CV R2 MEAN: 0.8188 | CV R2 STD: 0.0227\n"
     ]
    }
   ],
   "source": [
    "# BASELINE SVR MODEL WITH BEST HYPERPARAMETERS FROM OPTUNA STUDY\n",
    "\n",
    "C = 3474.2154702644193                        # BEST C FROM OPTUNA\n",
    "epsilon = 0.00011411129759067382              # BEST EPSILON\n",
    "gamma = \"scale\"                               # BEST GAMMA\n",
    "kernel = \"rbf\"                                # BEST KERNEL\n",
    "\n",
    "# REBUILD SVR PIPELINE WITH BEST PARAMS\n",
    "def build_svr_pipeline(C: float, epsilon: float, gamma: str, kernel: str) -> Pipeline:  # DEFINES PIPELINE\n",
    "    svr = SVR(C=C, epsilon=epsilon, gamma=gamma, kernel=kernel)                         # INITIALIZES SVR\n",
    "    mo = MultiOutputRegressor(svr)                                                      # WRAPS IN MULTI-OUTPUT\n",
    "    if USE_TARGET_SCALING:                                                              # IF SCALING TARGETS\n",
    "        mo = TransformedTargetRegressor(                                                # WRAPS TARGETS\n",
    "            regressor=mo, transformer=StandardScaler(with_mean=True, with_std=True)     # STANDARD SCALE TARGETS\n",
    "        )                                                                               # CLOSES TRANSFORMED TARGET\n",
    "    steps = [(\"pre\", preprocessor)]                                                     # ADDS PREPROCESSING\n",
    "    steps.append((\"reg\", mo))                                                           # ADDS REGRESSOR\n",
    "    return Pipeline(steps=steps)                                                        # BUILDS PIPELINE\n",
    "\n",
    "# BUILD AND EVALUATE FINAL BASELINE SVR PIPELINE\n",
    "svr_pipe_best = build_svr_pipeline(C=C, epsilon=epsilon, gamma=gamma, kernel=kernel)   # BUILDS BEST SVR\n",
    "svr_results = evaluate_sklearn_pipeline(\"best_svr\", svr_pipe_best)                     # EVALUATES AND STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 2\n",
      "=== LASSO_POLY_DEG_2 METRICS ===\n",
      "R2: 0.4494\n",
      "MAE: 2.4051\n",
      "MSE: 18.4633\n",
      "RMSE: 4.2969\n",
      "EXPLAINED_VARIANCE: 0.4499\n",
      "CV R2 MEAN: 0.5204 | CV R2 STD: 0.0174\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 3\n",
      "=== LASSO_POLY_DEG_3 METRICS ===\n",
      "R2: 0.5407\n",
      "MAE: 2.1684\n",
      "MSE: 16.6481\n",
      "RMSE: 4.0802\n",
      "EXPLAINED_VARIANCE: 0.5409\n",
      "CV R2 MEAN: 0.6196 | CV R2 STD: 0.0231\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 4\n",
      "=== LASSO_POLY_DEG_4 METRICS ===\n",
      "R2: 0.6367\n",
      "MAE: 1.9413\n",
      "MSE: 13.3212\n",
      "RMSE: 3.6498\n",
      "EXPLAINED_VARIANCE: 0.6372\n",
      "CV R2 MEAN: 0.6806 | CV R2 STD: 0.0224\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 5\n",
      "=== LASSO_POLY_DEG_5 METRICS ===\n",
      "R2: 0.7147\n",
      "MAE: 1.6238\n",
      "MSE: 9.6692\n",
      "RMSE: 3.1095\n",
      "EXPLAINED_VARIANCE: 0.7151\n",
      "CV R2 MEAN: 0.7449 | CV R2 STD: 0.0231\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 6\n",
      "=== LASSO_POLY_DEG_6 METRICS ===\n",
      "R2: 0.7438\n",
      "MAE: 1.4430\n",
      "MSE: 7.9425\n",
      "RMSE: 2.8183\n",
      "EXPLAINED_VARIANCE: 0.7442\n",
      "CV R2 MEAN: 0.7694 | CV R2 STD: 0.0260\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 7\n",
      "=== LASSO_POLY_DEG_7 METRICS ===\n",
      "R2: 0.7521\n",
      "MAE: 1.4080\n",
      "MSE: 7.6860\n",
      "RMSE: 2.7724\n",
      "EXPLAINED_VARIANCE: 0.7525\n",
      "CV R2 MEAN: 0.7765 | CV R2 STD: 0.0282\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 8\n",
      "=== LASSO_POLY_DEG_8 METRICS ===\n",
      "R2: 0.7565\n",
      "MAE: 1.3964\n",
      "MSE: 7.3670\n",
      "RMSE: 2.7142\n",
      "EXPLAINED_VARIANCE: 0.7569\n",
      "CV R2 MEAN: 0.7825 | CV R2 STD: 0.0270\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 9\n",
      "=== LASSO_POLY_DEG_9 METRICS ===\n",
      "R2: 0.7608\n",
      "MAE: 1.3790\n",
      "MSE: 7.1731\n",
      "RMSE: 2.6783\n",
      "EXPLAINED_VARIANCE: 0.7612\n",
      "CV R2 MEAN: 0.7869 | CV R2 STD: 0.0275\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 10\n",
      "=== LASSO_POLY_DEG_10 METRICS ===\n",
      "R2: 0.7625\n",
      "MAE: 1.3644\n",
      "MSE: 7.0823\n",
      "RMSE: 2.6613\n",
      "EXPLAINED_VARIANCE: 0.7628\n",
      "CV R2 MEAN: 0.7902 | CV R2 STD: 0.0290\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 11\n",
      "=== LASSO_POLY_DEG_11 METRICS ===\n",
      "R2: 0.7652\n",
      "MAE: 1.3431\n",
      "MSE: 6.9164\n",
      "RMSE: 2.6299\n",
      "EXPLAINED_VARIANCE: 0.7655\n",
      "CV R2 MEAN: 0.7927 | CV R2 STD: 0.0294\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 12\n",
      "=== LASSO_POLY_DEG_12 METRICS ===\n",
      "R2: 0.7655\n",
      "MAE: 1.3370\n",
      "MSE: 6.9111\n",
      "RMSE: 2.6289\n",
      "EXPLAINED_VARIANCE: 0.7659\n",
      "CV R2 MEAN: 0.7944 | CV R2 STD: 0.0295\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 13\n",
      "=== LASSO_POLY_DEG_13 METRICS ===\n",
      "R2: 0.7670\n",
      "MAE: 1.3277\n",
      "MSE: 6.8213\n",
      "RMSE: 2.6118\n",
      "EXPLAINED_VARIANCE: 0.7674\n",
      "CV R2 MEAN: 0.7954 | CV R2 STD: 0.0296\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 14\n",
      "=== LASSO_POLY_DEG_14 METRICS ===\n",
      "R2: 0.7681\n",
      "MAE: 1.3230\n",
      "MSE: 6.7824\n",
      "RMSE: 2.6043\n",
      "EXPLAINED_VARIANCE: 0.7684\n",
      "CV R2 MEAN: 0.7965 | CV R2 STD: 0.0298\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 15\n",
      "=== LASSO_POLY_DEG_15 METRICS ===\n",
      "R2: 0.7687\n",
      "MAE: 1.3143\n",
      "MSE: 6.7265\n",
      "RMSE: 2.5935\n",
      "EXPLAINED_VARIANCE: 0.7691\n",
      "CV R2 MEAN: 0.7972 | CV R2 STD: 0.0299\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 16\n",
      "=== LASSO_POLY_DEG_16 METRICS ===\n",
      "R2: 0.7689\n",
      "MAE: 1.3138\n",
      "MSE: 6.7244\n",
      "RMSE: 2.5931\n",
      "EXPLAINED_VARIANCE: 0.7693\n",
      "CV R2 MEAN: 0.7980 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 17\n",
      "=== LASSO_POLY_DEG_17 METRICS ===\n",
      "R2: 0.7693\n",
      "MAE: 1.3102\n",
      "MSE: 6.7030\n",
      "RMSE: 2.5890\n",
      "EXPLAINED_VARIANCE: 0.7697\n",
      "CV R2 MEAN: 0.7981 | CV R2 STD: 0.0300\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 18\n",
      "=== LASSO_POLY_DEG_18 METRICS ===\n",
      "R2: 0.7696\n",
      "MAE: 1.3081\n",
      "MSE: 6.6897\n",
      "RMSE: 2.5864\n",
      "EXPLAINED_VARIANCE: 0.7700\n",
      "CV R2 MEAN: 0.7984 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 19\n",
      "=== LASSO_POLY_DEG_19 METRICS ===\n",
      "R2: 0.7698\n",
      "MAE: 1.3066\n",
      "MSE: 6.6784\n",
      "RMSE: 2.5843\n",
      "EXPLAINED_VARIANCE: 0.7702\n",
      "CV R2 MEAN: 0.7984 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 20\n",
      "=== LASSO_POLY_DEG_20 METRICS ===\n",
      "R2: 0.7702\n",
      "MAE: 1.3035\n",
      "MSE: 6.6553\n",
      "RMSE: 2.5798\n",
      "EXPLAINED_VARIANCE: 0.7706\n",
      "CV R2 MEAN: 0.7986 | CV R2 STD: 0.0302\n"
     ]
    }
   ],
   "source": [
    "# LASSO REGRESSION (TRIAL 115 - RECREATE BEST CONFIGURATION AND HYPER-TUNE POLY DEGREE UP TO 20)\n",
    "\n",
    "results = []  # LIST TO COLLECT RESULTS\n",
    "\n",
    "for degree in range(2, 21):  # LOOP OVER POLYNOMIAL DEGREE FROM 2 TO 20\n",
    "    USE_POLYNOMIALS = True                             # ENABLE POLYNOMIAL FEATURES\n",
    "    POLY_DEGREE = degree                               # SET CURRENT POLY DEGREE\n",
    "    poly = PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)  # POLY TRANSFORMER\n",
    "    print(f\"POLYNOMIAL FEATURES ENABLED. DEGREE: {POLY_DEGREE}\")       # PRINT STATUS\n",
    "\n",
    "    # DEFINE NUMERIC PIPELINE BASED ON BEST TRIAL CONFIG\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),    # MEAN IMPUTATION (as per best config)\n",
    "        (\"scaler\", RobustScaler()),                     # ROBUST SCALING (as per best config)\n",
    "    ])\n",
    "\n",
    "    # APPLY TO FEATURE COLUMNS ONLY\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    # DEFINE LASSO REGRESSOR WITH BEST ALPHA FROM TRIAL 115\n",
    "    lasso = Lasso(alpha=0.0007944698383043709, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "    # WRAP IN TTR IF TARGET SCALING ENABLED\n",
    "    if USE_TARGET_SCALING:\n",
    "        lasso = TransformedTargetRegressor(\n",
    "            regressor=lasso, transformer=StandardScaler(with_mean=True, with_std=True)\n",
    "        )\n",
    "\n",
    "    # BUILD PIPELINE WITH PREPROCESSOR, POLY FEATURES, AND REGRESSOR\n",
    "    steps = [(\"pre\", preprocessor), (\"poly\", poly), (\"reg\", lasso)]\n",
    "    lasso_pipe = Pipeline(steps=steps)\n",
    "\n",
    "    # EVALUATE USING YOUR EVALUATION FUNCTION AND STORE RESULTS\n",
    "    model_name = f\"lasso_poly_deg_{POLY_DEGREE}\"\n",
    "    results.append(evaluate_sklearn_pipeline(model_name, lasso_pipe))  # APPEND EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 10\n",
      "=== LASSO_POLY_DEG_10 METRICS ===\n",
      "R2: 0.7625\n",
      "MAE: 1.3644\n",
      "MSE: 7.0823\n",
      "RMSE: 2.6613\n",
      "EXPLAINED_VARIANCE: 0.7628\n",
      "CV R2 MEAN: 0.7902 | CV R2 STD: 0.0290\n"
     ]
    }
   ],
   "source": [
    "POLY_DEGREE = 10                                                # SET POLY DEGREE TO 10\n",
    "poly = PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)  # POLY TRANSFORMER\n",
    "print(f\"POLYNOMIAL FEATURES ENABLED. DEGREE: {POLY_DEGREE}\")    # PRINT STATUS\n",
    "\n",
    "# NUMERIC PIPELINE BASED ON BEST TRIAL CONFIG\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),               # MEAN IMPUTATION (AS PER BEST CONFIG)\n",
    "    (\"scaler\", RobustScaler()),                                # ROBUST SCALING (AS PER BEST CONFIG)\n",
    "])\n",
    "\n",
    "# APPLY TO FEATURE COLUMNS ONLY\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# LASSO REGRESSOR WITH BEST ALPHA\n",
    "lasso = Lasso(alpha=0.0007944698383043709, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "# WRAP IN TTR IF TARGET SCALING ENABLED\n",
    "if USE_TARGET_SCALING:\n",
    "    lasso = TransformedTargetRegressor(\n",
    "        regressor=lasso, transformer=StandardScaler(with_mean=True, with_std=True)\n",
    "    )\n",
    "\n",
    "# BUILD PIPELINE WITH PREPROCESSOR, POLY FEATURES, AND REGRESSOR\n",
    "steps = [(\"pre\", preprocessor), (\"poly\", poly), (\"reg\", lasso)]\n",
    "lasso_pipe = Pipeline(steps=steps)\n",
    "\n",
    "# EVALUATE USING YOUR EVALUATION FUNCTION\n",
    "model_name = f\"lasso_poly_deg_{POLY_DEGREE}\"\n",
    "result = evaluate_sklearn_pipeline(model_name, lasso_pipe)     # RUN EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeef88d",
   "metadata": {},
   "source": [
    "### DECISION TREE REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd9c958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECISION_TREE METRICS ===\n",
      "R2: 0.8733\n",
      "MAE: 0.5478\n",
      "MSE: 2.1435\n",
      "RMSE: 1.4641\n",
      "EXPLAINED_VARIANCE: 0.8736\n",
      "CV R2 MEAN: 0.8939 | CV R2 STD: 0.0235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# DECISION TREE DOES NOT NEED SCALING\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[(\"imputer\", SimpleImputer(strategy=\"mean\"), FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=GLOBAL_SEED)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    tree = TransformedTargetRegressor(\n",
    "        regressor=tree, transformer=StandardScaler()\n",
    "    )\n",
    "\n",
    "tree_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_tree),\n",
    "    (\"reg\", tree)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"decision_tree\", tree_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661ce1b",
   "metadata": {},
   "source": [
    "### RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9dd625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RANDOM_FOREST METRICS ===\n",
      "R2: 0.8988\n",
      "MAE: 0.5503\n",
      "MSE: 1.7961\n",
      "RMSE: 1.3402\n",
      "EXPLAINED_VARIANCE: 0.8988\n",
      "CV R2 MEAN: 0.9278 | CV R2 STD: 0.0161\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[(\"imputer\", SimpleImputer(strategy=\"mean\"), FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    rf = TransformedTargetRegressor(regressor=rf, transformer=StandardScaler())\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_rf),\n",
    "    (\"reg\", rf)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"random_forest\", rf_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e077e76",
   "metadata": {},
   "source": [
    "### XGBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBOOST METRICS ===\n",
      "R2: 0.8906\n",
      "MAE: 0.6468\n",
      "MSE: 1.9189\n",
      "RMSE: 1.3852\n",
      "EXPLAINED_VARIANCE: 0.8907\n",
      "CV R2 MEAN: 0.9102 | CV R2 STD: 0.0172\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "preprocessor_xgb = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6,\n",
    "                   random_state=GLOBAL_SEED, n_jobs=-1, verbosity=0)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    xgb = TransformedTargetRegressor(regressor=xgb, transformer=StandardScaler())\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_xgb),\n",
    "    (\"reg\", xgb)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"xgboost\", xgb_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd1c86",
   "metadata": {},
   "source": [
    "### HISTGRADIENTBOOSTING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e50be1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HIST_GRADIENT_BOOSTING METRICS ===\n",
      "R2: 0.8837\n",
      "MAE: 0.8123\n",
      "MSE: 2.9495\n",
      "RMSE: 1.7174\n",
      "EXPLAINED_VARIANCE: 0.8837\n",
      "CV R2 MEAN: 0.8919 | CV R2 STD: 0.0154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "# BUILD HISTGRADIENTBOOSTING PIPELINE\n",
    "def build_hgb_pipeline(use_target_scaling: bool = False) -> Pipeline:\n",
    "    base_hgb = HistGradientBoostingRegressor(\n",
    "        max_iter=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=GLOBAL_SEED\n",
    "    )\n",
    "\n",
    "    # WRAP IN MULTIOUTPUTREGRESSOR TO SUPPORT MULTI-TARGET REGRESSION\n",
    "    mo_hgb = MultiOutputRegressor(base_hgb)\n",
    "\n",
    "    # OPTIONALLY SCALE TARGETS USING TRANSFORMEDTARGETREGRESSOR\n",
    "    if use_target_scaling:\n",
    "        mo_hgb = TransformedTargetRegressor(\n",
    "            regressor=mo_hgb,\n",
    "            transformer=StandardScaler()\n",
    "        )\n",
    "\n",
    "    # BUILD PIPELINE WITH COLUMN TRANSFORMER TO PASS FEATURES THROUGH\n",
    "    pipe = Pipeline([\n",
    "        (\"dropper\", ColumnTransformer([(\"num\", \"passthrough\", FEATURE_COLS)], remainder=\"drop\")),\n",
    "        (\"reg\", mo_hgb)\n",
    "    ])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# BUILD AND EVALUATE HISTGRADIENTBOOSTING PIPELINE\n",
    "hgb_pipe = build_hgb_pipeline(use_target_scaling=USE_TARGET_SCALING)\n",
    "results.append(evaluate_sklearn_pipeline(\"hist_gradient_boosting\", hgb_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01fdd2",
   "metadata": {},
   "source": [
    "### LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eef31e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "=== LIGHTGBM METRICS ===\n",
      "R2: 0.8924\n",
      "MAE: 0.7607\n",
      "MSE: 2.5348\n",
      "RMSE: 1.5921\n",
      "EXPLAINED_VARIANCE: 0.8925\n",
      "CV R2 MEAN: 0.8967 | CV R2 STD: 0.0168\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# SCALING FOR LGBM\n",
    "preprocessor_lgbm = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "base_lgbm = LGBMRegressor(n_estimators=100, learning_rate=0.1,\n",
    "                          max_depth=-1, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "\n",
    "# WRAP FOR MULTI-TARGET REGRESSION\n",
    "mo_lgbm = MultiOutputRegressor(base_lgbm)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    mo_lgbm = TransformedTargetRegressor(regressor=mo_lgbm, transformer=StandardScaler())\n",
    "\n",
    "lgbm_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_lgbm),\n",
    "    (\"reg\", mo_lgbm)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"lightgbm\", lgbm_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295a70a",
   "metadata": {},
   "source": [
    "### CATBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eee72ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATBOOST METRICS ===\n",
      "R2: 0.8847\n",
      "MAE: 0.7965\n",
      "MSE: 2.9470\n",
      "RMSE: 1.7167\n",
      "EXPLAINED_VARIANCE: 0.8848\n",
      "CV R2 MEAN: 0.8995 | CV R2 STD: 0.0156\n",
      "CATBOOST PIPELINE BUILT AND EVALUATED.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# CATBOOST \n",
    "base_cat = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=GLOBAL_SEED,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# WRAP CATBOOST IN MULTIOUTPUTREGRESSOR TO HANDLE MULTI-TARGET REGRESSION\n",
    "mo_cat = MultiOutputRegressor(base_cat)\n",
    "\n",
    "# SCALE TARGETS USING TRANSFORMEDTARGETREGRESSOR\n",
    "if USE_TARGET_SCALING:\n",
    "    mo_cat = TransformedTargetRegressor(\n",
    "        regressor=mo_cat,\n",
    "        transformer=StandardScaler()\n",
    "    )\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"DROPPER\", ColumnTransformer([(\"NUM\", \"passthrough\", FEATURE_COLS)], remainder=\"drop\")),\n",
    "    (\"REG\", mo_cat)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"catboost\", cat_pipe))\n",
    "\n",
    "print(\"CATBOOST PIPELINE BUILT AND EVALUATED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e138caf",
   "metadata": {},
   "source": [
    "### STACKING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2ebd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING REGRESSOR METRICS:\n",
      "R2: 0.8641\n",
      "MAE: 0.6315\n",
      "MSE: 2.9301\n",
      "RMSE: 1.7118\n",
      "EXPLAINED_VARIANCE: 0.8646\n",
      "CV R2 MEAN: 0.8923 | CV R2 STD: 0.0346\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING \n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, FEATURE_COLS)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# BASE MODELS \n",
    "def build_svr():\n",
    "    return (\"svr\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", SVR(C=3474.215, epsilon=0.0001141, gamma=\"scale\"))\n",
    "    ]))\n",
    "\n",
    "def build_rf():\n",
    "    return (\"rf\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "    ]))\n",
    "\n",
    "def build_xgb():\n",
    "    return (\"xgb\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "    ]))\n",
    "\n",
    "def build_lgbm():\n",
    "    return (\"lgbm\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=GLOBAL_SEED))\n",
    "    ]))\n",
    "\n",
    "estimators = [build_svr(), build_rf(), build_xgb(), build_lgbm()]\n",
    "\n",
    "# META LEARNER \n",
    "poly = PolynomialFeatures(degree=10, include_bias=False)\n",
    "lasso = Lasso(alpha=0.000794, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "meta_learner = Pipeline([\n",
    "    (\"poly\", poly),\n",
    "    (\"reg\", lasso)\n",
    "])\n",
    "\n",
    "# STACKING REGRESSOR \n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# WRAP IN MULTIOUTPUT REGRESSOR\n",
    "stacking_multioutput = MultiOutputRegressor(stacking, n_jobs=-1)\n",
    "\n",
    "# FINAL PIPELINE\n",
    "stacking_pipeline = Pipeline([\n",
    "    (\"stack\", stacking_multioutput)\n",
    "])\n",
    "\n",
    "# EVALUATE USING YOUR EVALUATION FUNCTION\n",
    "model_name = f\"STACKING REGRESSOR\"\n",
    "result = evaluate_sklearn_pipeline(model_name, stacking_pipeline)     # RUN EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee442a99",
   "metadata": {},
   "source": [
    "### STACKING REGRESSOR WITH POLYNOMIAL FEATURES AND FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8972328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING REGRESSOR METRICS:\n",
      "R2: 0.8389\n",
      "MAE: 0.8875\n",
      "MSE: 3.3806\n",
      "RMSE: 1.8387\n",
      "EXPLAINED_VARIANCE: 0.8391\n",
      "CV R2 MEAN: 0.8659 | CV R2 STD: 0.0190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# COMBINED PREPROCESSING STEP INCLUDING POLY FEATURES\n",
    "preprocessor = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", RobustScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=5, include_bias=False)),\n",
    "])\n",
    "\n",
    "# FEATURE SELECTION (AFTER POLY)\n",
    "feature_selector = SelectKBest(score_func=f_regression, k=20)\n",
    "\n",
    "# BASE MODEL BUILDERSNO INTERNAL PREPROCESSING\n",
    "def build_svr():\n",
    "    return (\"svr\", SVR(C=3474.215, epsilon=0.0001141, gamma=\"scale\"))\n",
    "\n",
    "def build_rf():\n",
    "    return (\"rf\", RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "\n",
    "def build_xgb():\n",
    "    return (\"xgb\", XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "\n",
    "def build_lgbm():\n",
    "    return (\"lgbm\", LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=GLOBAL_SEED))\n",
    "\n",
    "estimators = [build_svr(), build_rf(), build_xgb(), build_lgbm()]\n",
    "\n",
    "# META LEARNER (LASSO)\n",
    "lasso = Lasso(alpha=0.000794, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "# STACKING REGRESSOR WITH passthrough=True\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=lasso,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED),\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# MAIN PIPELINE: PREPROCESSING + SELECTKBEST + STACKING\n",
    "stacking_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", feature_selector),\n",
    "    (\"stack\", stacking)\n",
    "])\n",
    "\n",
    "# MULTI-OUTPUT WRAPPER\n",
    "stacking_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "# EVALUATE\n",
    "model_name = \"STACKING REGRESSOR\"\n",
    "result = evaluate_sklearn_pipeline(model_name, stacking_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe6e7c",
   "metadata": {},
   "source": [
    "### SELECTKBEST SWEEP FOR OPTIMAL K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7d4b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=20...\n",
      "STACKING REGRESSOR K=20 METRICS:\n",
      "R2: 0.8389\n",
      "MAE: 0.8875\n",
      "MSE: 3.3806\n",
      "RMSE: 1.8387\n",
      "EXPLAINED_VARIANCE: 0.8391\n",
      "CV R2 MEAN: 0.8659 | CV R2 STD: 0.0190\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=25...\n",
      "STACKING REGRESSOR K=25 METRICS:\n",
      "R2: 0.8403\n",
      "MAE: 0.8701\n",
      "MSE: 3.3433\n",
      "RMSE: 1.8285\n",
      "EXPLAINED_VARIANCE: 0.8405\n",
      "CV R2 MEAN: 0.8677 | CV R2 STD: 0.0180\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=30...\n",
      "STACKING REGRESSOR K=30 METRICS:\n",
      "R2: 0.8524\n",
      "MAE: 0.7934\n",
      "MSE: 2.6200\n",
      "RMSE: 1.6186\n",
      "EXPLAINED_VARIANCE: 0.8524\n",
      "CV R2 MEAN: 0.8745 | CV R2 STD: 0.0217\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=35...\n",
      "STACKING REGRESSOR K=35 METRICS:\n",
      "R2: 0.8707\n",
      "MAE: 0.7354\n",
      "MSE: 2.5418\n",
      "RMSE: 1.5943\n",
      "EXPLAINED_VARIANCE: 0.8707\n",
      "CV R2 MEAN: 0.8860 | CV R2 STD: 0.0225\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=40...\n",
      "STACKING REGRESSOR K=40 METRICS:\n",
      "R2: 0.8772\n",
      "MAE: 0.6847\n",
      "MSE: 2.1062\n",
      "RMSE: 1.4513\n",
      "EXPLAINED_VARIANCE: 0.8773\n",
      "CV R2 MEAN: 0.8915 | CV R2 STD: 0.0226\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "    K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "--------------------------------------------------------------------------------\n",
      "   20 |   0.8389 |   0.8875 |   3.3806 |   1.8387 |    0.8391 |     0.8659 |    0.0190\n",
      "   25 |   0.8403 |   0.8701 |   3.3433 |   1.8285 |    0.8405 |     0.8677 |    0.0180\n",
      "   30 |   0.8524 |   0.7934 |   2.6200 |   1.6186 |    0.8524 |     0.8745 |    0.0217\n",
      "   35 |   0.8707 |   0.7354 |   2.5418 |   1.5943 |    0.8707 |     0.8860 |    0.0225\n",
      "   40 |   0.8772 |   0.6847 |   2.1062 |   1.4513 |    0.8773 |     0.8915 |    0.0226\n"
     ]
    }
   ],
   "source": [
    "# RANGE OF K VALUES TO EVALUATE\n",
    "k_values = range(20, 45, 5)\n",
    "\n",
    "# LIST TO HOLD RESULTS FOR EACH K\n",
    "results_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nRUNNING PIPELINE WITH SELECTKBEST K={k}...\")\n",
    "\n",
    "    # UPDATE FEATURE SELECTOR WITH CURRENT K\n",
    "    feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "\n",
    "    # DEFINE THE PIPELINE FOR THIS K\n",
    "    stacking_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", feature_selector),\n",
    "        (\"stack\", stacking)\n",
    "    ])\n",
    "\n",
    "    # WRAP WITH MULTIOUTPUTREGRESSOR\n",
    "    multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "    # EVALUATE PIPELINE (YOUR EXISTING EVALUATION FUNCTION)\n",
    "    model_name = f\"STACKING REGRESSOR K={k}\"\n",
    "    result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "\n",
    "    # STORE RESULTS WITH K FOR LATER COMPARISON\n",
    "    results_list.append((k, result))\n",
    "\n",
    "# AFTER ALL RUNS, PRINT SUMMARIZED COMPARISON\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 80)\n",
    "for k, res in results_list:\n",
    "    print(f\"{k:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4921fb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=45...\n",
      "STACKING REGRESSOR K=45 METRICS:\n",
      "R2: 0.8821\n",
      "MAE: 0.6351\n",
      "MSE: 1.7173\n",
      "RMSE: 1.3104\n",
      "EXPLAINED_VARIANCE: 0.8821\n",
      "CV R2 MEAN: 0.8954 | CV R2 STD: 0.0218\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=50...\n",
      "STACKING REGRESSOR K=50 METRICS:\n",
      "R2: 0.8812\n",
      "MAE: 0.6506\n",
      "MSE: 1.7492\n",
      "RMSE: 1.3226\n",
      "EXPLAINED_VARIANCE: 0.8813\n",
      "CV R2 MEAN: 0.8969 | CV R2 STD: 0.0163\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=55...\n",
      "STACKING REGRESSOR K=55 METRICS:\n",
      "R2: 0.8873\n",
      "MAE: 0.6763\n",
      "MSE: 1.9567\n",
      "RMSE: 1.3988\n",
      "EXPLAINED_VARIANCE: 0.8876\n",
      "CV R2 MEAN: 0.9026 | CV R2 STD: 0.0095\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "    K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "--------------------------------------------------------------------------------\n",
      "   45 |   0.8821 |   0.6351 |   1.7173 |   1.3104 |    0.8821 |     0.8954 |    0.0218\n",
      "   50 |   0.8812 |   0.6506 |   1.7492 |   1.3226 |    0.8813 |     0.8969 |    0.0163\n",
      "   55 |   0.8873 |   0.6763 |   1.9567 |   1.3988 |    0.8876 |     0.9026 |    0.0095\n"
     ]
    }
   ],
   "source": [
    "# RANGE OF K VALUES TO EVALUATE\n",
    "k_values = [45, 50, 55]\n",
    "\n",
    "# LIST TO HOLD RESULTS FOR EACH K\n",
    "results_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nRUNNING PIPELINE WITH SELECTKBEST K={k}...\")\n",
    "\n",
    "    # UPDATE FEATURE SELECTOR WITH CURRENT K\n",
    "    feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "\n",
    "    # DEFINE THE PIPELINE FOR THIS K\n",
    "    stacking_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", feature_selector),\n",
    "        (\"stack\", stacking)\n",
    "    ])\n",
    "\n",
    "    # WRAP WITH MULTIOUTPUTREGRESSOR\n",
    "    multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "    # EVALUATE PIPELINE (YOUR EXISTING EVALUATION FUNCTION)\n",
    "    model_name = f\"STACKING REGRESSOR K={k}\"\n",
    "    result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "\n",
    "    # STORE RESULTS WITH K FOR LATER COMPARISON\n",
    "    results_list.append((k, result))\n",
    "\n",
    "# AFTER ALL RUNS, PRINT SUMMARIZED COMPARISON\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 80)\n",
    "for k, res in results_list:\n",
    "    print(f\"{k:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd7eaa",
   "metadata": {},
   "source": [
    "### POLYNOMIAL DEGREE SWEEP WITH SELECTKBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4dcd99b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING DEGREE=6, K=104 (FEATURES: 209)\n",
      "STACK DEG=6 K=104 METRICS:\n",
      "R2: 0.8877\n",
      "MAE: 0.6640\n",
      "MSE: 1.8643\n",
      "RMSE: 1.3654\n",
      "EXPLAINED_VARIANCE: 0.8878\n",
      "CV R2 MEAN: 0.9044 | CV R2 STD: 0.0132\n",
      "\n",
      "RUNNING DEGREE=6, K=124 (FEATURES: 209)\n",
      "STACK DEG=6 K=124 METRICS:\n",
      "R2: 0.8924\n",
      "MAE: 0.6543\n",
      "MSE: 1.9131\n",
      "RMSE: 1.3831\n",
      "EXPLAINED_VARIANCE: 0.8925\n",
      "CV R2 MEAN: 0.9077 | CV R2 STD: 0.0187\n",
      "\n",
      "RUNNING DEGREE=6, K=144 (FEATURES: 209)\n",
      "STACK DEG=6 K=144 METRICS:\n",
      "R2: 0.8919\n",
      "MAE: 0.6508\n",
      "MSE: 2.0180\n",
      "RMSE: 1.4206\n",
      "EXPLAINED_VARIANCE: 0.8920\n",
      "CV R2 MEAN: 0.9051 | CV R2 STD: 0.0201\n",
      "\n",
      "RUNNING DEGREE=6, K=164 (FEATURES: 209)\n",
      "STACK DEG=6 K=164 METRICS:\n",
      "R2: 0.8932\n",
      "MAE: 0.6373\n",
      "MSE: 1.8532\n",
      "RMSE: 1.3613\n",
      "EXPLAINED_VARIANCE: 0.8933\n",
      "CV R2 MEAN: 0.9076 | CV R2 STD: 0.0181\n",
      "\n",
      "RUNNING DEGREE=6, K=184 (FEATURES: 209)\n",
      "STACK DEG=6 K=184 METRICS:\n",
      "R2: 0.8899\n",
      "MAE: 0.6702\n",
      "MSE: 2.0711\n",
      "RMSE: 1.4391\n",
      "EXPLAINED_VARIANCE: 0.8900\n",
      "CV R2 MEAN: 0.9058 | CV R2 STD: 0.0186\n",
      "\n",
      "RUNNING DEGREE=6, K=204 (FEATURES: 209)\n",
      "STACK DEG=6 K=204 METRICS:\n",
      "R2: 0.8888\n",
      "MAE: 0.6777\n",
      "MSE: 2.1046\n",
      "RMSE: 1.4507\n",
      "EXPLAINED_VARIANCE: 0.8890\n",
      "CV R2 MEAN: 0.9049 | CV R2 STD: 0.0180\n",
      "\n",
      "RUNNING DEGREE=7, K=164 (FEATURES: 329)\n",
      "STACK DEG=7 K=164 METRICS:\n",
      "R2: 0.8843\n",
      "MAE: 0.6814\n",
      "MSE: 1.9423\n",
      "RMSE: 1.3937\n",
      "EXPLAINED_VARIANCE: 0.8845\n",
      "CV R2 MEAN: 0.9005 | CV R2 STD: 0.0115\n",
      "\n",
      "RUNNING DEGREE=7, K=196 (FEATURES: 329)\n",
      "STACK DEG=7 K=196 METRICS:\n",
      "R2: 0.8878\n",
      "MAE: 0.6637\n",
      "MSE: 1.9820\n",
      "RMSE: 1.4078\n",
      "EXPLAINED_VARIANCE: 0.8880\n",
      "CV R2 MEAN: 0.9038 | CV R2 STD: 0.0185\n",
      "\n",
      "RUNNING DEGREE=7, K=228 (FEATURES: 329)\n",
      "STACK DEG=7 K=228 METRICS:\n",
      "R2: 0.8884\n",
      "MAE: 0.6679\n",
      "MSE: 1.9751\n",
      "RMSE: 1.4054\n",
      "EXPLAINED_VARIANCE: 0.8886\n",
      "CV R2 MEAN: 0.9009 | CV R2 STD: 0.0198\n",
      "\n",
      "RUNNING DEGREE=7, K=260 (FEATURES: 329)\n",
      "STACK DEG=7 K=260 METRICS:\n",
      "R2: 0.8882\n",
      "MAE: 0.6753\n",
      "MSE: 2.0325\n",
      "RMSE: 1.4257\n",
      "EXPLAINED_VARIANCE: 0.8883\n",
      "CV R2 MEAN: 0.9032 | CV R2 STD: 0.0194\n",
      "\n",
      "RUNNING DEGREE=7, K=292 (FEATURES: 329)\n",
      "STACK DEG=7 K=292 METRICS:\n",
      "R2: 0.8872\n",
      "MAE: 0.6892\n",
      "MSE: 2.1158\n",
      "RMSE: 1.4546\n",
      "EXPLAINED_VARIANCE: 0.8874\n",
      "CV R2 MEAN: 0.9023 | CV R2 STD: 0.0196\n",
      "\n",
      "RUNNING DEGREE=7, K=324 (FEATURES: 329)\n",
      "STACK DEG=7 K=324 METRICS:\n",
      "R2: 0.8850\n",
      "MAE: 0.6950\n",
      "MSE: 2.1618\n",
      "RMSE: 1.4703\n",
      "EXPLAINED_VARIANCE: 0.8851\n",
      "CV R2 MEAN: 0.9013 | CV R2 STD: 0.0179\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      " DEG |     K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "------------------------------------------------------------------------------------------\n",
      "   6 |   104 |   0.8877 |   0.6640 |   1.8643 |   1.3654 |    0.8878 |     0.9044 |    0.0132\n",
      "   6 |   124 |   0.8924 |   0.6543 |   1.9131 |   1.3831 |    0.8925 |     0.9077 |    0.0187\n",
      "   6 |   144 |   0.8919 |   0.6508 |   2.0180 |   1.4206 |    0.8920 |     0.9051 |    0.0201\n",
      "   6 |   164 |   0.8932 |   0.6373 |   1.8532 |   1.3613 |    0.8933 |     0.9076 |    0.0181\n",
      "   6 |   184 |   0.8899 |   0.6702 |   2.0711 |   1.4391 |    0.8900 |     0.9058 |    0.0186\n",
      "   6 |   204 |   0.8888 |   0.6777 |   2.1046 |   1.4507 |    0.8890 |     0.9049 |    0.0180\n",
      "   7 |   164 |   0.8843 |   0.6814 |   1.9423 |   1.3937 |    0.8845 |     0.9005 |    0.0115\n",
      "   7 |   196 |   0.8878 |   0.6637 |   1.9820 |   1.4078 |    0.8880 |     0.9038 |    0.0185\n",
      "   7 |   228 |   0.8884 |   0.6679 |   1.9751 |   1.4054 |    0.8886 |     0.9009 |    0.0198\n",
      "   7 |   260 |   0.8882 |   0.6753 |   2.0325 |   1.4257 |    0.8883 |     0.9032 |    0.0194\n",
      "   7 |   292 |   0.8872 |   0.6892 |   2.1158 |   1.4546 |    0.8874 |     0.9023 |    0.0196\n",
      "   7 |   324 |   0.8850 |   0.6950 |   2.1618 |   1.4703 |    0.8851 |     0.9013 |    0.0179\n"
     ]
    }
   ],
   "source": [
    "# DEGREES\n",
    "degree_list = [6, 7]\n",
    "\n",
    "# LIST TO HOLD ALL RESULTS\n",
    "results_list = []\n",
    "\n",
    "for degree in degree_list:\n",
    "    # FIND ACTUAL NUMBER OF POLY FEATURES\n",
    "    n_input_features = len(FEATURE_COLS)\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    n_poly_features = poly.fit_transform(np.zeros((1, n_input_features))).shape[1]\n",
    "    \n",
    "    # CHOOSE K VALUES: FROM HALF TO ALL FEATURES, STEP BY 10% OF FEATURE COUNT\n",
    "    k_start = max(1, int(n_poly_features * 0.5))\n",
    "    k_stop = n_poly_features + 1\n",
    "    k_step = max(1, int(n_poly_features * 0.1))   \n",
    "    \n",
    "    k_values = list(range(k_start, k_stop, k_step))\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nRUNNING DEGREE={degree}, K={k} (FEATURES: {n_poly_features})\")\n",
    "\n",
    "        # SAFEGUARD: DON'T REQUEST MORE FEATURES THAN AVAILABLE\n",
    "        if k > n_poly_features:\n",
    "            continue\n",
    "\n",
    "        preprocessor = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", RobustScaler()),\n",
    "            (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ])\n",
    "        feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        stacking_pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"feature_selection\", feature_selector),\n",
    "            (\"stack\", stacking)\n",
    "        ])\n",
    "        multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "        model_name = f\"STACK DEG={degree} K={k}\"\n",
    "        result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "        results_list.append({'degree': degree, 'k': k, **result})\n",
    "\n",
    "# PRINT SUMMARIZED RESULTS\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'DEG':>4} | {'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 90)\n",
    "for res in results_list:\n",
    "    print(f\"{res['degree']:4} | {res['k']:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d7039",
   "metadata": {},
   "source": [
    "### POLYNOMIAL DEGREE 810 WITH SELECTKBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "639ad983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING DEGREE=8, K=148 (FEATURES: 494)\n",
      "STACK DEG=8 K=148 METRICS:\n",
      "R2: 0.8742\n",
      "MAE: 0.7153\n",
      "MSE: 2.1110\n",
      "RMSE: 1.4529\n",
      "EXPLAINED_VARIANCE: 0.8743\n",
      "CV R2 MEAN: 0.8799 | CV R2 STD: 0.0211\n",
      "\n",
      "RUNNING DEGREE=8, K=197 (FEATURES: 494)\n",
      "STACK DEG=8 K=197 METRICS:\n",
      "R2: 0.8843\n",
      "MAE: 0.6898\n",
      "MSE: 1.9433\n",
      "RMSE: 1.3940\n",
      "EXPLAINED_VARIANCE: 0.8845\n",
      "CV R2 MEAN: 0.8907 | CV R2 STD: 0.0214\n",
      "\n",
      "RUNNING DEGREE=8, K=246 (FEATURES: 494)\n",
      "STACK DEG=8 K=246 METRICS:\n",
      "R2: 0.8820\n",
      "MAE: 0.6889\n",
      "MSE: 1.9818\n",
      "RMSE: 1.4078\n",
      "EXPLAINED_VARIANCE: 0.8821\n",
      "CV R2 MEAN: 0.8997 | CV R2 STD: 0.0145\n",
      "\n",
      "RUNNING DEGREE=8, K=295 (FEATURES: 494)\n",
      "STACK DEG=8 K=295 METRICS:\n",
      "R2: 0.8856\n",
      "MAE: 0.6755\n",
      "MSE: 2.0730\n",
      "RMSE: 1.4398\n",
      "EXPLAINED_VARIANCE: 0.8856\n",
      "CV R2 MEAN: 0.9021 | CV R2 STD: 0.0172\n",
      "\n",
      "RUNNING DEGREE=8, K=344 (FEATURES: 494)\n",
      "STACK DEG=8 K=344 METRICS:\n",
      "R2: 0.8866\n",
      "MAE: 0.6850\n",
      "MSE: 2.0844\n",
      "RMSE: 1.4437\n",
      "EXPLAINED_VARIANCE: 0.8867\n",
      "CV R2 MEAN: 0.9002 | CV R2 STD: 0.0223\n",
      "\n",
      "RUNNING DEGREE=8, K=393 (FEATURES: 494)\n",
      "STACK DEG=8 K=393 METRICS:\n",
      "R2: 0.8840\n",
      "MAE: 0.6896\n",
      "MSE: 2.1979\n",
      "RMSE: 1.4825\n",
      "EXPLAINED_VARIANCE: 0.8841\n",
      "CV R2 MEAN: 0.9020 | CV R2 STD: 0.0211\n",
      "\n",
      "RUNNING DEGREE=8, K=442 (FEATURES: 494)\n",
      "STACK DEG=8 K=442 METRICS:\n",
      "R2: 0.8827\n",
      "MAE: 0.7077\n",
      "MSE: 2.2607\n",
      "RMSE: 1.5036\n",
      "EXPLAINED_VARIANCE: 0.8829\n",
      "CV R2 MEAN: 0.9003 | CV R2 STD: 0.0204\n",
      "\n",
      "RUNNING DEGREE=8, K=491 (FEATURES: 494)\n",
      "STACK DEG=8 K=491 METRICS:\n",
      "R2: 0.8835\n",
      "MAE: 0.6986\n",
      "MSE: 2.1850\n",
      "RMSE: 1.4782\n",
      "EXPLAINED_VARIANCE: 0.8838\n",
      "CV R2 MEAN: 0.9002 | CV R2 STD: 0.0210\n",
      "\n",
      "RUNNING DEGREE=9, K=214 (FEATURES: 714)\n",
      "STACK DEG=9 K=214 METRICS:\n",
      "R2: 0.8653\n",
      "MAE: 0.7533\n",
      "MSE: 2.5898\n",
      "RMSE: 1.6093\n",
      "EXPLAINED_VARIANCE: 0.8654\n",
      "CV R2 MEAN: 0.8754 | CV R2 STD: 0.0232\n",
      "\n",
      "RUNNING DEGREE=9, K=285 (FEATURES: 714)\n",
      "STACK DEG=9 K=285 METRICS:\n",
      "R2: 0.8830\n",
      "MAE: 0.6926\n",
      "MSE: 2.0133\n",
      "RMSE: 1.4189\n",
      "EXPLAINED_VARIANCE: 0.8832\n",
      "CV R2 MEAN: 0.8889 | CV R2 STD: 0.0247\n",
      "\n",
      "RUNNING DEGREE=9, K=356 (FEATURES: 714)\n",
      "STACK DEG=9 K=356 METRICS:\n",
      "R2: 0.8769\n",
      "MAE: 0.7168\n",
      "MSE: 2.2287\n",
      "RMSE: 1.4929\n",
      "EXPLAINED_VARIANCE: 0.8770\n",
      "CV R2 MEAN: 0.8966 | CV R2 STD: 0.0145\n",
      "\n",
      "RUNNING DEGREE=9, K=427 (FEATURES: 714)\n",
      "STACK DEG=9 K=427 METRICS:\n",
      "R2: 0.8831\n",
      "MAE: 0.6987\n",
      "MSE: 2.1386\n",
      "RMSE: 1.4624\n",
      "EXPLAINED_VARIANCE: 0.8833\n",
      "CV R2 MEAN: 0.8990 | CV R2 STD: 0.0190\n",
      "\n",
      "RUNNING DEGREE=9, K=498 (FEATURES: 714)\n",
      "STACK DEG=9 K=498 METRICS:\n",
      "R2: 0.8813\n",
      "MAE: 0.7213\n",
      "MSE: 2.3042\n",
      "RMSE: 1.5180\n",
      "EXPLAINED_VARIANCE: 0.8814\n",
      "CV R2 MEAN: 0.8981 | CV R2 STD: 0.0218\n",
      "\n",
      "RUNNING DEGREE=9, K=569 (FEATURES: 714)\n",
      "STACK DEG=9 K=569 METRICS:\n",
      "R2: 0.8825\n",
      "MAE: 0.7055\n",
      "MSE: 2.2921\n",
      "RMSE: 1.5140\n",
      "EXPLAINED_VARIANCE: 0.8826\n",
      "CV R2 MEAN: 0.8993 | CV R2 STD: 0.0215\n",
      "\n",
      "RUNNING DEGREE=9, K=640 (FEATURES: 714)\n",
      "STACK DEG=9 K=640 METRICS:\n",
      "R2: 0.8805\n",
      "MAE: 0.7250\n",
      "MSE: 2.3516\n",
      "RMSE: 1.5335\n",
      "EXPLAINED_VARIANCE: 0.8807\n",
      "CV R2 MEAN: 0.8990 | CV R2 STD: 0.0199\n",
      "\n",
      "RUNNING DEGREE=9, K=711 (FEATURES: 714)\n",
      "STACK DEG=9 K=711 METRICS:\n",
      "R2: 0.8786\n",
      "MAE: 0.7353\n",
      "MSE: 2.4125\n",
      "RMSE: 1.5532\n",
      "EXPLAINED_VARIANCE: 0.8787\n",
      "CV R2 MEAN: 0.8986 | CV R2 STD: 0.0192\n",
      "\n",
      "RUNNING DEGREE=10, K=300 (FEATURES: 1000)\n",
      "STACK DEG=10 K=300 METRICS:\n",
      "R2: 0.8625\n",
      "MAE: 0.7734\n",
      "MSE: 2.7424\n",
      "RMSE: 1.6560\n",
      "EXPLAINED_VARIANCE: 0.8627\n",
      "CV R2 MEAN: 0.8766 | CV R2 STD: 0.0242\n",
      "\n",
      "RUNNING DEGREE=10, K=400 (FEATURES: 1000)\n",
      "STACK DEG=10 K=400 METRICS:\n",
      "R2: 0.8816\n",
      "MAE: 0.6984\n",
      "MSE: 2.0722\n",
      "RMSE: 1.4395\n",
      "EXPLAINED_VARIANCE: 0.8817\n",
      "CV R2 MEAN: 0.8890 | CV R2 STD: 0.0242\n",
      "\n",
      "RUNNING DEGREE=10, K=500 (FEATURES: 1000)\n",
      "STACK DEG=10 K=500 METRICS:\n",
      "R2: 0.8776\n",
      "MAE: 0.6901\n",
      "MSE: 2.1312\n",
      "RMSE: 1.4599\n",
      "EXPLAINED_VARIANCE: 0.8778\n",
      "CV R2 MEAN: 0.8927 | CV R2 STD: 0.0134\n",
      "\n",
      "RUNNING DEGREE=10, K=600 (FEATURES: 1000)\n",
      "STACK DEG=10 K=600 METRICS:\n",
      "R2: 0.8792\n",
      "MAE: 0.7096\n",
      "MSE: 2.2844\n",
      "RMSE: 1.5114\n",
      "EXPLAINED_VARIANCE: 0.8794\n",
      "CV R2 MEAN: 0.8968 | CV R2 STD: 0.0191\n",
      "\n",
      "RUNNING DEGREE=10, K=700 (FEATURES: 1000)\n",
      "STACK DEG=10 K=700 METRICS:\n",
      "R2: 0.8795\n",
      "MAE: 0.7115\n",
      "MSE: 2.2683\n",
      "RMSE: 1.5061\n",
      "EXPLAINED_VARIANCE: 0.8797\n",
      "CV R2 MEAN: 0.8967 | CV R2 STD: 0.0225\n",
      "\n",
      "RUNNING DEGREE=10, K=800 (FEATURES: 1000)\n",
      "STACK DEG=10 K=800 METRICS:\n",
      "R2: 0.8831\n",
      "MAE: 0.7053\n",
      "MSE: 2.3090\n",
      "RMSE: 1.5195\n",
      "EXPLAINED_VARIANCE: 0.8833\n",
      "CV R2 MEAN: 0.8974 | CV R2 STD: 0.0236\n",
      "\n",
      "RUNNING DEGREE=10, K=900 (FEATURES: 1000)\n",
      "STACK DEG=10 K=900 METRICS:\n",
      "R2: 0.8794\n",
      "MAE: 0.7285\n",
      "MSE: 2.3895\n",
      "RMSE: 1.5458\n",
      "EXPLAINED_VARIANCE: 0.8796\n",
      "CV R2 MEAN: 0.8976 | CV R2 STD: 0.0229\n",
      "\n",
      "RUNNING DEGREE=10, K=1000 (FEATURES: 1000)\n",
      "STACK DEG=10 K=1000 METRICS:\n",
      "R2: 0.8778\n",
      "MAE: 0.7249\n",
      "MSE: 2.4255\n",
      "RMSE: 1.5574\n",
      "EXPLAINED_VARIANCE: 0.8780\n",
      "CV R2 MEAN: 0.8975 | CV R2 STD: 0.0214\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      " DEG |     K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "------------------------------------------------------------------------------------------\n",
      "   8 |   148 |   0.8742 |   0.7153 |   2.1110 |   1.4529 |    0.8743 |     0.8799 |    0.0211\n",
      "   8 |   197 |   0.8843 |   0.6898 |   1.9433 |   1.3940 |    0.8845 |     0.8907 |    0.0214\n",
      "   8 |   246 |   0.8820 |   0.6889 |   1.9818 |   1.4078 |    0.8821 |     0.8997 |    0.0145\n",
      "   8 |   295 |   0.8856 |   0.6755 |   2.0730 |   1.4398 |    0.8856 |     0.9021 |    0.0172\n",
      "   8 |   344 |   0.8866 |   0.6850 |   2.0844 |   1.4437 |    0.8867 |     0.9002 |    0.0223\n",
      "   8 |   393 |   0.8840 |   0.6896 |   2.1979 |   1.4825 |    0.8841 |     0.9020 |    0.0211\n",
      "   8 |   442 |   0.8827 |   0.7077 |   2.2607 |   1.5036 |    0.8829 |     0.9003 |    0.0204\n",
      "   8 |   491 |   0.8835 |   0.6986 |   2.1850 |   1.4782 |    0.8838 |     0.9002 |    0.0210\n",
      "   9 |   214 |   0.8653 |   0.7533 |   2.5898 |   1.6093 |    0.8654 |     0.8754 |    0.0232\n",
      "   9 |   285 |   0.8830 |   0.6926 |   2.0133 |   1.4189 |    0.8832 |     0.8889 |    0.0247\n",
      "   9 |   356 |   0.8769 |   0.7168 |   2.2287 |   1.4929 |    0.8770 |     0.8966 |    0.0145\n",
      "   9 |   427 |   0.8831 |   0.6987 |   2.1386 |   1.4624 |    0.8833 |     0.8990 |    0.0190\n",
      "   9 |   498 |   0.8813 |   0.7213 |   2.3042 |   1.5180 |    0.8814 |     0.8981 |    0.0218\n",
      "   9 |   569 |   0.8825 |   0.7055 |   2.2921 |   1.5140 |    0.8826 |     0.8993 |    0.0215\n",
      "   9 |   640 |   0.8805 |   0.7250 |   2.3516 |   1.5335 |    0.8807 |     0.8990 |    0.0199\n",
      "   9 |   711 |   0.8786 |   0.7353 |   2.4125 |   1.5532 |    0.8787 |     0.8986 |    0.0192\n",
      "  10 |   300 |   0.8625 |   0.7734 |   2.7424 |   1.6560 |    0.8627 |     0.8766 |    0.0242\n",
      "  10 |   400 |   0.8816 |   0.6984 |   2.0722 |   1.4395 |    0.8817 |     0.8890 |    0.0242\n",
      "  10 |   500 |   0.8776 |   0.6901 |   2.1312 |   1.4599 |    0.8778 |     0.8927 |    0.0134\n",
      "  10 |   600 |   0.8792 |   0.7096 |   2.2844 |   1.5114 |    0.8794 |     0.8968 |    0.0191\n",
      "  10 |   700 |   0.8795 |   0.7115 |   2.2683 |   1.5061 |    0.8797 |     0.8967 |    0.0225\n",
      "  10 |   800 |   0.8831 |   0.7053 |   2.3090 |   1.5195 |    0.8833 |     0.8974 |    0.0236\n",
      "  10 |   900 |   0.8794 |   0.7285 |   2.3895 |   1.5458 |    0.8796 |     0.8976 |    0.0229\n",
      "  10 |  1000 |   0.8778 |   0.7249 |   2.4255 |   1.5574 |    0.8780 |     0.8975 |    0.0214\n"
     ]
    }
   ],
   "source": [
    "# DEGREES\n",
    "degree_list = [8, 9, 10]\n",
    "\n",
    "# LIST TO HOLD ALL RESULTS\n",
    "results_list = []\n",
    "\n",
    "for degree in degree_list:\n",
    "    # FIND ACTUAL NUMBER OF POLY FEATURES\n",
    "    n_input_features = len(FEATURE_COLS)\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    n_poly_features = poly.fit_transform(np.zeros((1, n_input_features))).shape[1]\n",
    "    \n",
    "    # CHOOSE K VALUES: FROM LESS THAN HALF TO ALL FEATURES, STEP BY 30% OF FEATURE COUNT\n",
    "    k_start = max(1, int(n_poly_features * 0.3))\n",
    "    k_stop = n_poly_features + 1\n",
    "    k_step = max(1, int(n_poly_features * 0.1))   \n",
    "    \n",
    "    k_values = list(range(k_start, k_stop, k_step))\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nRUNNING DEGREE={degree}, K={k} (FEATURES: {n_poly_features})\")\n",
    "\n",
    "        # SAFEGUARD: DON'T REQUEST MORE FEATURES THAN AVAILABLE\n",
    "        if k > n_poly_features:\n",
    "            continue\n",
    "\n",
    "        preprocessor = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", RobustScaler()),\n",
    "            (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ])\n",
    "        feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        stacking_pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"feature_selection\", feature_selector),\n",
    "            (\"stack\", stacking)\n",
    "        ])\n",
    "        multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "        model_name = f\"STACK DEG={degree} K={k}\"\n",
    "        result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "        results_list.append({'degree': degree, 'k': k, **result})\n",
    "\n",
    "# PRINT SUMMARIZED RESULTS\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'DEG':>4} | {'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 90)\n",
    "for res in results_list:\n",
    "    print(f\"{res['degree']:4} | {res['k']:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62123a7b",
   "metadata": {},
   "source": [
    "### SELECTKBEST TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90b27d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL POLYNOMIAL FEATURES (DEGREE=6): 209\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=160...\n",
      "STACKING REGRESSOR K=160 METRICS:\n",
      "R2: 0.8915\n",
      "MAE: 0.6446\n",
      "MSE: 1.9151\n",
      "RMSE: 1.3839\n",
      "EXPLAINED_VARIANCE: 0.8917\n",
      "CV R2 MEAN: 0.9076 | CV R2 STD: 0.0187\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=162...\n",
      "STACKING REGRESSOR K=162 METRICS:\n",
      "R2: 0.8922\n",
      "MAE: 0.6465\n",
      "MSE: 1.9653\n",
      "RMSE: 1.4019\n",
      "EXPLAINED_VARIANCE: 0.8923\n",
      "CV R2 MEAN: 0.9074 | CV R2 STD: 0.0182\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=164...\n",
      "STACKING REGRESSOR K=164 METRICS:\n",
      "R2: 0.8932\n",
      "MAE: 0.6373\n",
      "MSE: 1.8532\n",
      "RMSE: 1.3613\n",
      "EXPLAINED_VARIANCE: 0.8933\n",
      "CV R2 MEAN: 0.9076 | CV R2 STD: 0.0181\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=166...\n",
      "STACKING REGRESSOR K=166 METRICS:\n",
      "R2: 0.8901\n",
      "MAE: 0.6557\n",
      "MSE: 1.9523\n",
      "RMSE: 1.3972\n",
      "EXPLAINED_VARIANCE: 0.8902\n",
      "CV R2 MEAN: 0.9073 | CV R2 STD: 0.0179\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=168...\n",
      "STACKING REGRESSOR K=168 METRICS:\n",
      "R2: 0.8901\n",
      "MAE: 0.6568\n",
      "MSE: 1.9682\n",
      "RMSE: 1.4029\n",
      "EXPLAINED_VARIANCE: 0.8902\n",
      "CV R2 MEAN: 0.9077 | CV R2 STD: 0.0185\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=170...\n",
      "STACKING REGRESSOR K=170 METRICS:\n",
      "R2: 0.8906\n",
      "MAE: 0.6677\n",
      "MSE: 2.0379\n",
      "RMSE: 1.4275\n",
      "EXPLAINED_VARIANCE: 0.8907\n",
      "CV R2 MEAN: 0.9080 | CV R2 STD: 0.0178\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=172...\n",
      "STACKING REGRESSOR K=172 METRICS:\n",
      "R2: 0.8895\n",
      "MAE: 0.6620\n",
      "MSE: 2.0489\n",
      "RMSE: 1.4314\n",
      "EXPLAINED_VARIANCE: 0.8897\n",
      "CV R2 MEAN: 0.9076 | CV R2 STD: 0.0193\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=174...\n",
      "STACKING REGRESSOR K=174 METRICS:\n",
      "R2: 0.8904\n",
      "MAE: 0.6657\n",
      "MSE: 2.0667\n",
      "RMSE: 1.4376\n",
      "EXPLAINED_VARIANCE: 0.8906\n",
      "CV R2 MEAN: 0.9073 | CV R2 STD: 0.0193\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=176...\n",
      "STACKING REGRESSOR K=176 METRICS:\n",
      "R2: 0.8903\n",
      "MAE: 0.6711\n",
      "MSE: 2.0427\n",
      "RMSE: 1.4292\n",
      "EXPLAINED_VARIANCE: 0.8905\n",
      "CV R2 MEAN: 0.9073 | CV R2 STD: 0.0186\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=178...\n",
      "STACKING REGRESSOR K=178 METRICS:\n",
      "R2: 0.8915\n",
      "MAE: 0.6551\n",
      "MSE: 1.9465\n",
      "RMSE: 1.3952\n",
      "EXPLAINED_VARIANCE: 0.8917\n",
      "CV R2 MEAN: 0.9069 | CV R2 STD: 0.0189\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=180...\n",
      "STACKING REGRESSOR K=180 METRICS:\n",
      "R2: 0.8922\n",
      "MAE: 0.6570\n",
      "MSE: 1.9357\n",
      "RMSE: 1.3913\n",
      "EXPLAINED_VARIANCE: 0.8924\n",
      "CV R2 MEAN: 0.9064 | CV R2 STD: 0.0185\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=182...\n",
      "STACKING REGRESSOR K=182 METRICS:\n",
      "R2: 0.8920\n",
      "MAE: 0.6571\n",
      "MSE: 1.9971\n",
      "RMSE: 1.4132\n",
      "EXPLAINED_VARIANCE: 0.8921\n",
      "CV R2 MEAN: 0.9063 | CV R2 STD: 0.0184\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "    K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "--------------------------------------------------------------------------------\n",
      "  164 |   0.8932 |   0.6373 |   1.8532 |   1.3613 |    0.8933 |     0.9076 |    0.0181\n",
      "  162 |   0.8922 |   0.6465 |   1.9653 |   1.4019 |    0.8923 |     0.9074 |    0.0182\n",
      "  180 |   0.8922 |   0.6570 |   1.9357 |   1.3913 |    0.8924 |     0.9064 |    0.0185\n",
      "  182 |   0.8920 |   0.6571 |   1.9971 |   1.4132 |    0.8921 |     0.9063 |    0.0184\n",
      "  178 |   0.8915 |   0.6551 |   1.9465 |   1.3952 |    0.8917 |     0.9069 |    0.0189\n",
      "  160 |   0.8915 |   0.6446 |   1.9151 |   1.3839 |    0.8917 |     0.9076 |    0.0187\n",
      "  170 |   0.8906 |   0.6677 |   2.0379 |   1.4275 |    0.8907 |     0.9080 |    0.0178\n",
      "  174 |   0.8904 |   0.6657 |   2.0667 |   1.4376 |    0.8906 |     0.9073 |    0.0193\n",
      "  176 |   0.8903 |   0.6711 |   2.0427 |   1.4292 |    0.8905 |     0.9073 |    0.0186\n",
      "  168 |   0.8901 |   0.6568 |   1.9682 |   1.4029 |    0.8902 |     0.9077 |    0.0185\n",
      "  166 |   0.8901 |   0.6557 |   1.9523 |   1.3972 |    0.8902 |     0.9073 |    0.0179\n",
      "  172 |   0.8895 |   0.6620 |   2.0489 |   1.4314 |    0.8897 |     0.9076 |    0.0193\n"
     ]
    }
   ],
   "source": [
    "n_input_features = len(FEATURE_COLS)\n",
    "poly = PolynomialFeatures(degree=6, include_bias=False)\n",
    "n_poly_features = poly.fit_transform(np.zeros((1, n_input_features))).shape[1]\n",
    "\n",
    "# TOTAL POLY FEATURES \n",
    "print(f\"TOTAL POLYNOMIAL FEATURES (DEGREE=6): {n_poly_features}\")\n",
    "\n",
    "# RANGE OF K VALUES TO EVALUATE\n",
    "k_values = range(160, 184, 2)\n",
    "\n",
    "# LIST TO HOLD RESULTS FOR EACH K\n",
    "results_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nRUNNING PIPELINE WITH SELECTKBEST K={k}...\")\n",
    "\n",
    "    # UPDATE FEATURE SELECTOR WITH CURRENT K\n",
    "    feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "\n",
    "    # COMBINED PREPROCESSING STEP INCLUDING POLY FEATURES\n",
    "    preprocessor = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"poly\", PolynomialFeatures(degree=6, include_bias=False)),\n",
    "    ])\n",
    "\n",
    "    # DEFINE THE PIPELINE FOR THIS K\n",
    "    stacking_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", feature_selector),\n",
    "        (\"stack\", stacking)\n",
    "    ])\n",
    "\n",
    "    # WRAP WITH MULTIOUTPUTREGRESSOR\n",
    "    multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "    # EVALUATE PIPELINE (YOUR EXISTING EVALUATION FUNCTION)\n",
    "    model_name = f\"STACKING REGRESSOR K={k}\"\n",
    "    result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "\n",
    "    # STORE RESULTS WITH K FOR LATER COMPARISON\n",
    "    results_list.append((k, result))\n",
    "\n",
    "# AFTER ALL RUNS, PRINT SUMMARIZED COMPARISON\n",
    "results_list_sorted = sorted(results_list, key=lambda x: x[1]['r2'], reverse=True)\n",
    "\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 80)\n",
    "for k, res in results_list_sorted:\n",
    "    print(f\"{k:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
