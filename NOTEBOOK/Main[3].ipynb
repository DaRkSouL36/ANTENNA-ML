{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ea3778",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223115b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n",
      "OS: Windows-11-10.0.22631-SP0\n",
      "NUMPY: 2.1.3\n",
      "PANDAS: 2.3.1\n",
      "SCIKIT-LEARN: 1.7.1\n",
      "MATPLOTLIB: 3.10.3\n",
      "SEABORN: 0.13.2\n",
      "PLOTLY: 6.2.0\n",
      "OPTUNA: 4.4.0\n",
      "SHAP: 0.48.0\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND PRINTS ALL VERSIONS SO WE CAN REPRODUCE RESULTS EXACTLY LATER              \n",
    "\n",
    "import sys                                           # STANDARD LIB TO ACCESS PYTHON RUNTIME DETAILS\n",
    "import platform                                      # STANDARD LIB TO GET OS/PLATFORM INFORMATION\n",
    "import warnings                                      # STANDARD LIB TO CONTROL WARNING MESSAGES\n",
    "import os                                            # STANDARD LIB TO QUERY CPU COUNT FOR PARALLELISM\n",
    "from pathlib import Path                             # STANDARD LIB FOR SAFE, CROSS-PLATFORM PATH HANDLING\n",
    "from typing import Dict, Any, Tuple                  # TYPE HINTS FOR CLARITY\n",
    "\n",
    "import numpy as np                                   # NUMERICAL COMPUTING\n",
    "import pandas as pd                                  # DATAFRAMES AND DATA MANIPULATION\n",
    "\n",
    "import matplotlib                                    # BASE PLOTTING BACKEND\n",
    "import matplotlib.pyplot as plt                      # STATEFUL PLOTTING INTERFACE\n",
    "from matplotlib import rcParams                      # IMPORTS RCPARAMS TO SET GLOBAL STYLES\n",
    "import seaborn as sns                                # STATISTICAL PLOTTING BUILT ON TOP OF MATPLOTLIB\n",
    "import plotly                                        # INTERACTIVE PLOTTING (NOT USED HERE BUT KEPT FOR CONSISTENCY)\n",
    "\n",
    "import sklearn                                       # SCIKIT-LEARN: CLASSIC MACHINE LEARNING\n",
    "from sklearn.model_selection import train_test_split # TRAIN/TEST SPLITTING\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate  # K-FOLD SPLITTING AND CV\n",
    "from sklearn.pipeline import Pipeline                # TO BUILD CLEAN, REUSABLE PREPROCESSING PIPELINES\n",
    "from sklearn.impute import SimpleImputer             # SIMPLE STRATEGIES TO IMPUTE MISSING VALUES\n",
    "from sklearn.preprocessing import (                  # SCALERS + TRANSFORMS\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    PolynomialFeatures, PowerTransformer\n",
    ")  \n",
    "from sklearn.compose import ColumnTransformer        # APPLY TRANSFORMS TO COLUMNS (WE USE ALL NUMERIC)\n",
    "from sklearn.metrics import (                        # METRICS FOR REGRESSION\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    explained_variance_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor    # WRAPS SINGLE-OUTPUT MODELS FOR MULTI-OUTPUT TARGETS\n",
    "from sklearn.linear_model import (                      # LINEAR FAMILY\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet\n",
    ")  \n",
    "from sklearn.neighbors import KNeighborsRegressor       # KNN REGRESSOR\n",
    "from sklearn.svm import SVR                             # SUPPORT VECTOR REGRESSION\n",
    "from sklearn.preprocessing import PolynomialFeatures    # POLYNOMIAL FEATURES\n",
    "from sklearn.compose import TransformedTargetRegressor  # FOR SCALING TARGETS\n",
    "from sklearn.ensemble import StackingRegressor          # FOR STACKING REGRESSOR\n",
    "\n",
    "\n",
    "import optuna                                        # HYPERPARAMETER OPTIMIZATION\n",
    "from optuna.samplers import TPESampler               # ADVANCED SAMPLER\n",
    "from optuna.pruners import MedianPruner              # EARLY STOPPING PRUNER\n",
    "import shap                                          # MODEL INTERPRETABILITY \n",
    "import joblib                                        # SERIALIZATION (SAVING/LOADING MODELS)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")                    # SUPPRESSES NON-CRITICAL WARNINGS FOR CLEANER OUTPUT\n",
    "\n",
    "print(\"PYTHON:\", sys.version)                        # PRINTS PYTHON VERSION\n",
    "print(\"OS:\", platform.platform())                    # PRINTS OPERATING SYSTEM DETAILS\n",
    "print(\"NUMPY:\", np.__version__)                      # PRINTS NUMPY VERSION\n",
    "print(\"PANDAS:\", pd.__version__)                     # PRINTS PANDAS VERSION\n",
    "print(\"SCIKIT-LEARN:\", sklearn.__version__)          # PRINTS SCIKIT-LEARN VERSION\n",
    "print(\"MATPLOTLIB:\", matplotlib.__version__)         # PRINTS MATPLOTLIB VERSION\n",
    "print(\"SEABORN:\", sns.__version__)                   # PRINTS SEABORN VERSION\n",
    "print(\"PLOTLY:\", plotly.__version__)                 # PRINTS PLOTLY VERSION\n",
    "print(\"OPTUNA:\", optuna.__version__)                 # PRINTS OPTUNA VERSION\n",
    "print(\"SHAP:\", shap.__version__)                     # PRINTS SHAP VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d951d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\antenna-ml-thz\\venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\antenna-ml-thz\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "SCIKIT-LEARN: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -U scikit-learn\n",
    "print(\"SCIKIT-LEARN:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dc27c",
   "metadata": {},
   "source": [
    "### PLOTTING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708af0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PLOTTING STYLE TO KEEP ALL FIGURES CRISP, BOLD, UPPERCASE, AND HIGH-IMPACT      \n",
    "FIGSIZE = (8, 6)                                      # DEFAULT FIGURE SIZE\n",
    "DPI = 500                                             # HIGH RESOLUTION\n",
    "LINEWIDTH = 2.0                                       # BOLD LINE WIDTH\n",
    "GRID_LINEWIDTH = 1.5                                  # BOLD GRID LINES\n",
    "FONTSIZE_TITLE = 20                                   # LARGE TITLE SIZE\n",
    "FONTSIZE_LABEL = 16                                   # LARGE AXIS LABEL SIZE\n",
    "FONTSIZE_TICK = 14                                    # LARGE TICK LABEL SIZE\n",
    "FONTSIZE_LEGEND = 14                                  # LARGE LEGEND FONT SIZE\n",
    "\n",
    "def init_plot_style() -> None:                        # DEFINES A FUNCTION TO INITIALIZE GLOBAL STYLE\n",
    "    \"\"\"SET GLOBAL MATPLOTLIB STYLE FOR BOLD, UPPERCASE, HIGH-RES FIGURES.\"\"\"  \n",
    "    rcParams[\"figure.figsize\"] = FIGSIZE              # SETS FIGURE SIZE\n",
    "    rcParams[\"figure.dpi\"] = DPI                      # SETS DPI\n",
    "    rcParams[\"savefig.dpi\"] = DPI                     # HIGH-RES SAVED FIGURES\n",
    "    rcParams[\"font.weight\"] = \"bold\"                  # MAKES TEXT BOLD\n",
    "    rcParams[\"axes.titleweight\"] = \"bold\"             # BOLD TITLES\n",
    "    rcParams[\"axes.labelweight\"] = \"bold\"             # BOLD LABELS\n",
    "    rcParams[\"axes.titlesize\"] = FONTSIZE_TITLE       # TITLE FONT SIZE\n",
    "    rcParams[\"axes.labelsize\"] = FONTSIZE_LABEL       # LABEL FONT SIZE\n",
    "    rcParams[\"xtick.labelsize\"] = FONTSIZE_TICK       # X-TICK LABEL SIZE\n",
    "    rcParams[\"ytick.labelsize\"] = FONTSIZE_TICK       # Y-TICK LABEL SIZE\n",
    "    rcParams[\"legend.fontsize\"] = FONTSIZE_LEGEND     # LEGEND FONT SIZE\n",
    "    rcParams[\"legend.title_fontsize\"] = FONTSIZE_LEGEND  # LEGEND TITLE FONT SIZE\n",
    "    rcParams[\"lines.linewidth\"] = LINEWIDTH           # DEFAULT LINE WIDTH\n",
    "    rcParams[\"grid.linewidth\"] = GRID_LINEWIDTH       # GRID LINE WIDTH\n",
    "    rcParams[\"axes.grid\"] = True                      # ENABLE GRID BY DEFAULT\n",
    "    rcParams[\"grid.alpha\"] = 0.3                      # GRID TRANSPARENCY\n",
    "    rcParams[\"axes.spines.top\"] = True                # SHOW TOP SPINE\n",
    "    rcParams[\"axes.spines.right\"] = True              # SHOW RIGHT SPINE\n",
    "\n",
    "def boldify_axes(ax: plt.Axes,\n",
    "                 title: str = \"\",\n",
    "                 xlabel: str = \"\",\n",
    "                 ylabel: str = \"\",\n",
    "                 legend: bool = True) -> None:\n",
    "    \"\"\"UPPERCASE + BOLD ALL TEXT ELEMENTS ON AN AXES OBJECT.\"\"\"  \n",
    "    if title:                                                    # CHECKS IF TITLE IS PROVIDED\n",
    "        ax.set_title(title.upper(), weight=\"bold\", size=FONTSIZE_TITLE)    # SETS BOLD, UPPERCASE TITLE\n",
    "    if xlabel:                                                   # CHECKS IF XLABEL IS PROVIDED\n",
    "        ax.set_xlabel(xlabel.upper(), weight=\"bold\", size=FONTSIZE_LABEL)  # SETS BOLD, UPPERCASE XLABEL\n",
    "    if ylabel:                                                   # CHECKS IF YLABEL IS PROVIDED\n",
    "        ax.set_ylabel(ylabel.upper(), weight=\"bold\", size=FONTSIZE_LABEL)  # SETS BOLD, UPPERCASE YLABEL\n",
    "\n",
    "    for tick in ax.get_xticklabels():                             # LOOPS OVER X TICKS\n",
    "        tick.set_fontweight(\"bold\")                               # MAKES THEM BOLD\n",
    "        tick.set_fontsize(FONTSIZE_TICK)                          # SETS FONT SIZE\n",
    "        tick.set_text(str(tick.get_text()).upper())               # UPPERCASES TEXT\n",
    "\n",
    "    for tick in ax.get_yticklabels():                             # LOOPS OVER Y TICKS\n",
    "        tick.set_fontweight(\"bold\")                               # MAKES THEM BOLD\n",
    "        tick.set_fontsize(FONTSIZE_TICK)                          # SETS FONT SIZE\n",
    "        tick.set_text(str(tick.get_text()).upper())               # UPPERCASES TEXT\n",
    "\n",
    "    for spine in ax.spines.values():                              # ITERATES OVER SPINES\n",
    "        spine.set_linewidth(2.0)                                  # MAKES SPINES THICK\n",
    "\n",
    "    if legend and ax.get_legend() is not None:                     # IF LEGEND EXISTS, FORMAT IT\n",
    "        leg = ax.get_legend()                                      # GETS LEGEND HANDLE\n",
    "        if leg.get_title() is not None:                            # IF LEGEND TITLE EXISTS\n",
    "            leg.get_title().set_text(leg.get_title().get_text().upper())  # UPPERCASE LEGEND TITLE\n",
    "            leg.get_title().set_fontweight(\"bold\")                 # BOLD LEGEND TITLE\n",
    "        for text in leg.get_texts():                               # FOR EACH LEGEND LABEL\n",
    "            text.set_text(text.get_text().upper())                 # UPPERCASE TEXT\n",
    "            text.set_fontweight(\"bold\")                            # BOLD TEXT\n",
    "            text.set_fontsize(FONTSIZE_LEGEND)                     # SET FONT SIZE\n",
    "\n",
    "def finalize_figure(fig: plt.Figure, suptitle: str = \"\") -> None:\n",
    "    \"\"\"APPLY SUPTITLE (UPPERCASE, BOLD) AND TIGHT LAYOUT.\"\"\"       \n",
    "    if suptitle:                                                   # IF SUPTITLE PROVIDED\n",
    "        fig.suptitle(suptitle.upper(), fontsize=FONTSIZE_TITLE, fontweight=\"bold\")  # SETS BOLD, UPPERCASE SUPTITLE\n",
    "    fig.tight_layout()                                             # TIGHT LAYOUT TO PREVENT CLIPPING\n",
    "\n",
    "init_plot_style()                                                  # INITIALIZES THE GLOBAL STYLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415cecb1",
   "metadata": {},
   "source": [
    "### CONFIG & REPRODUCIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2e00c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL SEED: 42\n",
      "TRAIN-TEST SPLIT: 0.8 0.2\n",
      "K-FOLDS: 5\n",
      "N_TRIALS: 1000 | N_JOBS: 5\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION FOR REPRODUCIBILITY AND PROJECT-SPECIFIC CONSTANTS                       \n",
    "\n",
    "GLOBAL_SEED = 42                                     # GLOBAL SEED FOR REPRODUCIBILITY\n",
    "np.random.seed(GLOBAL_SEED)                          # SETS NUMPY SEED\n",
    "\n",
    "# PATHS (ADAPT LOCALLY): THE USER REQUESTED ..\\DATA\\DATA[P].csv                          \n",
    "DATA_CSV_PATH = Path(\"../DATA/DATA[P].csv\")          # RELATIVE PATH AS SPECIFIED BY USER\n",
    "RESULTS_DIR = Path(\"../DATA/\")                        # DIRECTORY TO SAVE RESULTS\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)       # CREATES DIRECTORY IF NOT EXISTS\n",
    "\n",
    "FEATURE_COLS = [\"il\", \"iw\", \"pw\", \"ro\"]              # FEATURES FROM USER\n",
    "TARGET_COLS = [\"frequency\", \"return loss\", \"gain\"]   # MULTI-OUTPUT TARGETS FROM USER\n",
    "\n",
    "TEST_SIZE = 0.2                                       # 80/20 TRAIN-TEST SPLIT\n",
    "N_SPLITS = 5                                          # 5-FOLD CV\n",
    "OPTUNA_TRIALS = 100                                   # OPTUNA TRIAL BUDGET\n",
    "N_TRIALS = 1000                                       # VERY HIGH-BUDGET SEARCH \n",
    "N_JOBS = max(1, (os.cpu_count() or 2) - 1)            # PARALLEL TRIALS USING AVAILABLE CORES MINUS ONE\n",
    "USE_TARGET_SCALING = True                             # SCALE TARGETS\n",
    "USE_POWER_TRANSFORM = True                            # ALLOW YEO-JOHNSON AS A TUNABLE OPTION\n",
    "USE_POLYNOMIALS = True                                # ALLOW POLYNOMIAL FEATURES AS A TUNABLE OPTION\n",
    "POLY_DEGREE = 2                                       # DEGREE 2 POLYNOMIALS \n",
    "MAX_POLY_DEGREE = 5                                   # UPPER BOUND FOR POLYNOMIAL DEGREE\n",
    "\n",
    "print(\"GLOBAL SEED:\", GLOBAL_SEED)                   # CONFIRMS GLOBAL SEED\n",
    "print(\"TRAIN-TEST SPLIT:\", 1 - TEST_SIZE, TEST_SIZE) # PRINTS TRAIN/TEST RATIO\n",
    "print(\"K-FOLDS:\", N_SPLITS)                          # PRINTS NUMBER OF FOLDS\n",
    "print(\"N_TRIALS:\", N_TRIALS, \"| N_JOBS:\", N_JOBS)    # PRINTS TRIALS AND PARALLEL JOBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f79651",
   "metadata": {},
   "source": [
    "### DATA READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0c5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADED: (1296, 7)\n"
     ]
    }
   ],
   "source": [
    "# SAFE CSV READER TO LOAD THE CLEANED DATA                                               \n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:       # DEFINES A SAFE CSV READER\n",
    "    \"\"\"SAFELY READ A CSV FILE AND RETURN A PANDAS DATAFRAME WITH CLEAR ERRORS.\"\"\"  \n",
    "    if not path.exists():                            # CHECKS IF FILE EXISTS\n",
    "        raise FileNotFoundError(f\"FILE NOT FOUND: {path}\")  # RAISES ERROR IF NOT FOUND\n",
    "    df_local = pd.read_csv(path)                     # READS CSV\n",
    "    if df_local.empty:                               # CHECKS IF EMPTY\n",
    "        raise ValueError(\"THE CSV FILE IS EMPTY.\")   # RAISES ERROR IF EMPTY\n",
    "    return df_local                                  # RETURNS DATAFRAME\n",
    "\n",
    "df = safe_read_csv(DATA_CSV_PATH)                    # LOADS THE CLEANED DATA\n",
    "print(\"DATA LOADED:\", df.shape)                      # PRINTS SHAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc38b6",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT & K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66be0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE (X, Y): (1036, 4) (1036, 3)\n",
      "TEST SHAPE  (X, Y): (260, 4) (260, 3)\n",
      "K-FOLD READY.\n"
     ]
    }
   ],
   "source": [
    "# WE SPLIT ONCE INTO TRAIN/TEST (80/20), THEN USE K-FOLD ON THE TRAIN SET FOR CV        \n",
    "\n",
    "X = df[FEATURE_COLS]                          # EXTRACTS FEATURES AS NUMPY ARRAY\n",
    "Y = df[TARGET_COLS]                           # EXTRACTS TARGETS AS NUMPY ARRAY\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( # PERFORMS TRAIN-TEST SPLIT\n",
    "    X, Y, test_size=TEST_SIZE, random_state=GLOBAL_SEED, shuffle=True\n",
    ")                                                    \n",
    "\n",
    "print(\"TRAIN SHAPE (X, Y):\", X_train.shape, y_train.shape)  # PRINTS TRAIN SHAPES\n",
    "print(\"TEST SHAPE  (X, Y):\", X_test.shape, y_test.shape)    # PRINTS TEST SHAPES\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=GLOBAL_SEED)  # DEFINES K-FOLD SPLITTER\n",
    "print(\"K-FOLD READY.\")                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226c373",
   "metadata": {},
   "source": [
    "### METRICS HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904e8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY TO CALCULATE ALL REQUESTED METRICS IN ONE PLACE                                  \n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]: # DEFINES METRICS FUNCTION\n",
    "    \"\"\"RETURN A DICTIONARY OF REGRESSION METRICS FOR MULTI-OUTPUT TARGETS.\"\"\"       \n",
    "    r2 = r2_score(y_true, y_pred, multioutput=\"uniform_average\")                    # COMPUTES R2\n",
    "    mae = mean_absolute_error(y_true, y_pred, multioutput=\"uniform_average\")        # COMPUTES MAE\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput=\"uniform_average\")         # COMPUTES MSE\n",
    "    rmse = np.sqrt(mse)                                                             # COMPUTES RMSE\n",
    "    evs = explained_variance_score(y_true, y_pred, multioutput=\"uniform_average\")   # COMPUTES EXPLAINED VARIANCE\n",
    "    return {\"r2\": r2, \"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"explained_variance\": evs}  # RETURNS ALL METRICS\n",
    "\n",
    "def print_metrics(name: str, metrics: Dict[str, float]) -> None:      # DEFINES PRINTING FUNCTION\n",
    "    \"\"\"PRETTY-PRINT METRICS WITH UPPERCASE KEYS.\"\"\"                    \n",
    "    print(f\"=== {name.upper()} METRICS ===\")                          # PRINTS HEADER\n",
    "    for k, v in metrics.items():                                      # LOOPS OVER METRICS\n",
    "        print(f\"{k.upper()}: {v:.4f}\")                                # PRINTS EACH METRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b25ab",
   "metadata": {},
   "source": [
    "### PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a575b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSORS READY. TARGET SCALING: True\n"
     ]
    }
   ],
   "source": [
    "# NUMERIC PIPELINE FOR FEATURES: IMPUTE MEDIAN + STANDARD SCALER                         \n",
    "\n",
    "numeric_transformer = Pipeline(steps=[               # BUILDS A PIPELINE\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # MISSING VALUE IMPUTATION\n",
    "    (\"scaler\", StandardScaler()),                    # STANDARD SCALING\n",
    "])                                                   # CLOSES PIPELINE\n",
    "\n",
    "preprocessor = ColumnTransformer(                    # WRAPS TRANSFORMER (ALL COLUMNS ARE NUMERIC)\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],  # APPLIES TO ALL FEATURE COLS\n",
    "    remainder=\"drop\"                                 # DROPS ANY OTHER COLUMNS (THERE ARE NONE)\n",
    ")                                                    \n",
    "\n",
    "y_scaler = StandardScaler() if USE_TARGET_SCALING else None  # CREATES Y SCALER IF REQUESTED\n",
    "\n",
    "print(\"PREPROCESSORS READY. TARGET SCALING:\", USE_TARGET_SCALING)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa03cb1",
   "metadata": {},
   "source": [
    "### CROSS-VALIDATION EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a6336bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER TO RUN K-FOLD CV AND RETURN MEAN/STD R2, PLUS FIT ON FULL TRAIN AND EVAL TEST     \n",
    "\n",
    "def evaluate_sklearn_pipeline(name: str, pipe: Pipeline) -> Dict[str, Any]:  # DEFINES EVALUATION FUNCTION\n",
    "    \"\"\"FIT/VALIDATE A SKLEARN PIPELINE WITH K-FOLD AND TEST EVALUATION.\"\"\"   \n",
    "    # CROSS-VALIDATION R2 SCORES                                                     \n",
    "    cv_scores = cross_val_score(pipe, X_train, y_train, cv=kf, scoring=\"r2\", n_jobs=-1)  # RUNS CV\n",
    "    pipe.fit(X_train, y_train)                                                    # FITS ON FULL TRAIN\n",
    "    y_pred_test = pipe.predict(X_test)                                            # PREDICTS ON TEST\n",
    "    metrics = regression_metrics(y_test, y_pred_test)                             # COMPUTES METRICS\n",
    "    result = {                                                                    # BUILDS RESULT DICT\n",
    "        \"model\": name,\n",
    "        \"cv_r2_mean\": np.mean(cv_scores),\n",
    "        \"cv_r2_std\": np.std(cv_scores),\n",
    "        **metrics\n",
    "    }                                                                              # CLOSES DICT\n",
    "    print_metrics(name, metrics)                                                   # PRINTS TEST METRICS\n",
    "    print(f\"CV R2 MEAN: {result['cv_r2_mean']:.4f} | CV R2 STD: {result['cv_r2_std']:.4f}\")  # PRINTS CV SCORES\n",
    "    return result                                                                   # RETURNS RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20b2be",
   "metadata": {},
   "source": [
    "### SVR MODEL WITH BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dadde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEST_SVR METRICS ===\n",
      "R2: 0.8381\n",
      "MAE: 0.9642\n",
      "MSE: 4.4526\n",
      "RMSE: 2.1101\n",
      "EXPLAINED_VARIANCE: 0.8388\n",
      "CV R2 MEAN: 0.8188 | CV R2 STD: 0.0227\n"
     ]
    }
   ],
   "source": [
    "# BASELINE SVR MODEL WITH BEST HYPERPARAMETERS FROM OPTUNA STUDY\n",
    "\n",
    "C = 3474.2154702644193                        # BEST C FROM OPTUNA\n",
    "epsilon = 0.00011411129759067382              # BEST EPSILON\n",
    "gamma = \"scale\"                               # BEST GAMMA\n",
    "kernel = \"rbf\"                                # BEST KERNEL\n",
    "\n",
    "# REBUILD SVR PIPELINE WITH BEST PARAMS\n",
    "def build_svr_pipeline(C: float, epsilon: float, gamma: str, kernel: str) -> Pipeline:  # DEFINES PIPELINE\n",
    "    svr = SVR(C=C, epsilon=epsilon, gamma=gamma, kernel=kernel)                         # INITIALIZES SVR\n",
    "    mo = MultiOutputRegressor(svr)                                                      # WRAPS IN MULTI-OUTPUT\n",
    "    if USE_TARGET_SCALING:                                                              # IF SCALING TARGETS\n",
    "        mo = TransformedTargetRegressor(                                                # WRAPS TARGETS\n",
    "            regressor=mo, transformer=StandardScaler(with_mean=True, with_std=True)     # STANDARD SCALE TARGETS\n",
    "        )                                                                               # CLOSES TRANSFORMED TARGET\n",
    "    steps = [(\"pre\", preprocessor)]                                                     # ADDS PREPROCESSING\n",
    "    steps.append((\"reg\", mo))                                                           # ADDS REGRESSOR\n",
    "    return Pipeline(steps=steps)                                                        # BUILDS PIPELINE\n",
    "\n",
    "# BUILD AND EVALUATE FINAL BASELINE SVR PIPELINE\n",
    "svr_pipe_best = build_svr_pipeline(C=C, epsilon=epsilon, gamma=gamma, kernel=kernel)   # BUILDS BEST SVR\n",
    "svr_results = evaluate_sklearn_pipeline(\"best_svr\", svr_pipe_best)                     # EVALUATES AND STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 2\n",
      "=== LASSO_POLY_DEG_2 METRICS ===\n",
      "R2: 0.4494\n",
      "MAE: 2.4051\n",
      "MSE: 18.4633\n",
      "RMSE: 4.2969\n",
      "EXPLAINED_VARIANCE: 0.4499\n",
      "CV R2 MEAN: 0.5204 | CV R2 STD: 0.0174\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 3\n",
      "=== LASSO_POLY_DEG_3 METRICS ===\n",
      "R2: 0.5407\n",
      "MAE: 2.1684\n",
      "MSE: 16.6481\n",
      "RMSE: 4.0802\n",
      "EXPLAINED_VARIANCE: 0.5409\n",
      "CV R2 MEAN: 0.6196 | CV R2 STD: 0.0231\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 4\n",
      "=== LASSO_POLY_DEG_4 METRICS ===\n",
      "R2: 0.6367\n",
      "MAE: 1.9413\n",
      "MSE: 13.3212\n",
      "RMSE: 3.6498\n",
      "EXPLAINED_VARIANCE: 0.6372\n",
      "CV R2 MEAN: 0.6806 | CV R2 STD: 0.0224\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 5\n",
      "=== LASSO_POLY_DEG_5 METRICS ===\n",
      "R2: 0.7147\n",
      "MAE: 1.6238\n",
      "MSE: 9.6692\n",
      "RMSE: 3.1095\n",
      "EXPLAINED_VARIANCE: 0.7151\n",
      "CV R2 MEAN: 0.7449 | CV R2 STD: 0.0231\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 6\n",
      "=== LASSO_POLY_DEG_6 METRICS ===\n",
      "R2: 0.7438\n",
      "MAE: 1.4430\n",
      "MSE: 7.9425\n",
      "RMSE: 2.8183\n",
      "EXPLAINED_VARIANCE: 0.7442\n",
      "CV R2 MEAN: 0.7694 | CV R2 STD: 0.0260\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 7\n",
      "=== LASSO_POLY_DEG_7 METRICS ===\n",
      "R2: 0.7521\n",
      "MAE: 1.4080\n",
      "MSE: 7.6860\n",
      "RMSE: 2.7724\n",
      "EXPLAINED_VARIANCE: 0.7525\n",
      "CV R2 MEAN: 0.7765 | CV R2 STD: 0.0282\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 8\n",
      "=== LASSO_POLY_DEG_8 METRICS ===\n",
      "R2: 0.7565\n",
      "MAE: 1.3964\n",
      "MSE: 7.3670\n",
      "RMSE: 2.7142\n",
      "EXPLAINED_VARIANCE: 0.7569\n",
      "CV R2 MEAN: 0.7825 | CV R2 STD: 0.0270\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 9\n",
      "=== LASSO_POLY_DEG_9 METRICS ===\n",
      "R2: 0.7608\n",
      "MAE: 1.3790\n",
      "MSE: 7.1731\n",
      "RMSE: 2.6783\n",
      "EXPLAINED_VARIANCE: 0.7612\n",
      "CV R2 MEAN: 0.7869 | CV R2 STD: 0.0275\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 10\n",
      "=== LASSO_POLY_DEG_10 METRICS ===\n",
      "R2: 0.7625\n",
      "MAE: 1.3644\n",
      "MSE: 7.0823\n",
      "RMSE: 2.6613\n",
      "EXPLAINED_VARIANCE: 0.7628\n",
      "CV R2 MEAN: 0.7902 | CV R2 STD: 0.0290\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 11\n",
      "=== LASSO_POLY_DEG_11 METRICS ===\n",
      "R2: 0.7652\n",
      "MAE: 1.3431\n",
      "MSE: 6.9164\n",
      "RMSE: 2.6299\n",
      "EXPLAINED_VARIANCE: 0.7655\n",
      "CV R2 MEAN: 0.7927 | CV R2 STD: 0.0294\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 12\n",
      "=== LASSO_POLY_DEG_12 METRICS ===\n",
      "R2: 0.7655\n",
      "MAE: 1.3370\n",
      "MSE: 6.9111\n",
      "RMSE: 2.6289\n",
      "EXPLAINED_VARIANCE: 0.7659\n",
      "CV R2 MEAN: 0.7944 | CV R2 STD: 0.0295\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 13\n",
      "=== LASSO_POLY_DEG_13 METRICS ===\n",
      "R2: 0.7670\n",
      "MAE: 1.3277\n",
      "MSE: 6.8213\n",
      "RMSE: 2.6118\n",
      "EXPLAINED_VARIANCE: 0.7674\n",
      "CV R2 MEAN: 0.7954 | CV R2 STD: 0.0296\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 14\n",
      "=== LASSO_POLY_DEG_14 METRICS ===\n",
      "R2: 0.7681\n",
      "MAE: 1.3230\n",
      "MSE: 6.7824\n",
      "RMSE: 2.6043\n",
      "EXPLAINED_VARIANCE: 0.7684\n",
      "CV R2 MEAN: 0.7965 | CV R2 STD: 0.0298\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 15\n",
      "=== LASSO_POLY_DEG_15 METRICS ===\n",
      "R2: 0.7687\n",
      "MAE: 1.3143\n",
      "MSE: 6.7265\n",
      "RMSE: 2.5935\n",
      "EXPLAINED_VARIANCE: 0.7691\n",
      "CV R2 MEAN: 0.7972 | CV R2 STD: 0.0299\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 16\n",
      "=== LASSO_POLY_DEG_16 METRICS ===\n",
      "R2: 0.7689\n",
      "MAE: 1.3138\n",
      "MSE: 6.7244\n",
      "RMSE: 2.5931\n",
      "EXPLAINED_VARIANCE: 0.7693\n",
      "CV R2 MEAN: 0.7980 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 17\n",
      "=== LASSO_POLY_DEG_17 METRICS ===\n",
      "R2: 0.7693\n",
      "MAE: 1.3102\n",
      "MSE: 6.7030\n",
      "RMSE: 2.5890\n",
      "EXPLAINED_VARIANCE: 0.7697\n",
      "CV R2 MEAN: 0.7981 | CV R2 STD: 0.0300\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 18\n",
      "=== LASSO_POLY_DEG_18 METRICS ===\n",
      "R2: 0.7696\n",
      "MAE: 1.3081\n",
      "MSE: 6.6897\n",
      "RMSE: 2.5864\n",
      "EXPLAINED_VARIANCE: 0.7700\n",
      "CV R2 MEAN: 0.7984 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 19\n",
      "=== LASSO_POLY_DEG_19 METRICS ===\n",
      "R2: 0.7698\n",
      "MAE: 1.3066\n",
      "MSE: 6.6784\n",
      "RMSE: 2.5843\n",
      "EXPLAINED_VARIANCE: 0.7702\n",
      "CV R2 MEAN: 0.7984 | CV R2 STD: 0.0301\n",
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 20\n",
      "=== LASSO_POLY_DEG_20 METRICS ===\n",
      "R2: 0.7702\n",
      "MAE: 1.3035\n",
      "MSE: 6.6553\n",
      "RMSE: 2.5798\n",
      "EXPLAINED_VARIANCE: 0.7706\n",
      "CV R2 MEAN: 0.7986 | CV R2 STD: 0.0302\n"
     ]
    }
   ],
   "source": [
    "# LASSO REGRESSION (TRIAL 115 - RECREATE BEST CONFIGURATION AND HYPER-TUNE POLY DEGREE UP TO 20)\n",
    "\n",
    "results = []  # LIST TO COLLECT RESULTS\n",
    "\n",
    "for degree in range(2, 21):  # LOOP OVER POLYNOMIAL DEGREE FROM 2 TO 20\n",
    "    USE_POLYNOMIALS = True                             # ENABLE POLYNOMIAL FEATURES\n",
    "    POLY_DEGREE = degree                               # SET CURRENT POLY DEGREE\n",
    "    poly = PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)  # POLY TRANSFORMER\n",
    "    print(f\"POLYNOMIAL FEATURES ENABLED. DEGREE: {POLY_DEGREE}\")       # PRINT STATUS\n",
    "\n",
    "    # DEFINE NUMERIC PIPELINE BASED ON BEST TRIAL CONFIG\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),    # MEAN IMPUTATION (as per best config)\n",
    "        (\"scaler\", RobustScaler()),                     # ROBUST SCALING (as per best config)\n",
    "    ])\n",
    "\n",
    "    # APPLY TO FEATURE COLUMNS ONLY\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    # DEFINE LASSO REGRESSOR WITH BEST ALPHA FROM TRIAL 115\n",
    "    lasso = Lasso(alpha=0.0007944698383043709, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "    # WRAP IN TTR IF TARGET SCALING ENABLED\n",
    "    if USE_TARGET_SCALING:\n",
    "        lasso = TransformedTargetRegressor(\n",
    "            regressor=lasso, transformer=StandardScaler(with_mean=True, with_std=True)\n",
    "        )\n",
    "\n",
    "    # BUILD PIPELINE WITH PREPROCESSOR, POLY FEATURES, AND REGRESSOR\n",
    "    steps = [(\"pre\", preprocessor), (\"poly\", poly), (\"reg\", lasso)]\n",
    "    lasso_pipe = Pipeline(steps=steps)\n",
    "\n",
    "    # EVALUATE USING YOUR EVALUATION FUNCTION AND STORE RESULTS\n",
    "    model_name = f\"lasso_poly_deg_{POLY_DEGREE}\"\n",
    "    results.append(evaluate_sklearn_pipeline(model_name, lasso_pipe))  # APPEND EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL FEATURES ENABLED. DEGREE: 10\n",
      "=== LASSO_POLY_DEG_10 METRICS ===\n",
      "R2: 0.7625\n",
      "MAE: 1.3644\n",
      "MSE: 7.0823\n",
      "RMSE: 2.6613\n",
      "EXPLAINED_VARIANCE: 0.7628\n",
      "CV R2 MEAN: 0.7902 | CV R2 STD: 0.0290\n"
     ]
    }
   ],
   "source": [
    "POLY_DEGREE = 10                                                # SET POLY DEGREE TO 10\n",
    "poly = PolynomialFeatures(degree=POLY_DEGREE, include_bias=False)  # POLY TRANSFORMER\n",
    "print(f\"POLYNOMIAL FEATURES ENABLED. DEGREE: {POLY_DEGREE}\")    # PRINT STATUS\n",
    "\n",
    "# NUMERIC PIPELINE BASED ON BEST TRIAL CONFIG\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),               # MEAN IMPUTATION (AS PER BEST CONFIG)\n",
    "    (\"scaler\", RobustScaler()),                                # ROBUST SCALING (AS PER BEST CONFIG)\n",
    "])\n",
    "\n",
    "# APPLY TO FEATURE COLUMNS ONLY\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# LASSO REGRESSOR WITH BEST ALPHA\n",
    "lasso = Lasso(alpha=0.0007944698383043709, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "# WRAP IN TTR IF TARGET SCALING ENABLED\n",
    "if USE_TARGET_SCALING:\n",
    "    lasso = TransformedTargetRegressor(\n",
    "        regressor=lasso, transformer=StandardScaler(with_mean=True, with_std=True)\n",
    "    )\n",
    "\n",
    "# BUILD PIPELINE WITH PREPROCESSOR, POLY FEATURES, AND REGRESSOR\n",
    "steps = [(\"pre\", preprocessor), (\"poly\", poly), (\"reg\", lasso)]\n",
    "lasso_pipe = Pipeline(steps=steps)\n",
    "\n",
    "# EVALUATE USING YOUR EVALUATION FUNCTION\n",
    "model_name = f\"lasso_poly_deg_{POLY_DEGREE}\"\n",
    "result = evaluate_sklearn_pipeline(model_name, lasso_pipe)     # RUN EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeef88d",
   "metadata": {},
   "source": [
    "### DECISION TREE REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd9c958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DECISION_TREE METRICS ===\n",
      "R2: 0.8733\n",
      "MAE: 0.5478\n",
      "MSE: 2.1435\n",
      "RMSE: 1.4641\n",
      "EXPLAINED_VARIANCE: 0.8736\n",
      "CV R2 MEAN: 0.8939 | CV R2 STD: 0.0235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# DECISION TREE DOES NOT NEED SCALING\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[(\"imputer\", SimpleImputer(strategy=\"mean\"), FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=GLOBAL_SEED)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    tree = TransformedTargetRegressor(\n",
    "        regressor=tree, transformer=StandardScaler()\n",
    "    )\n",
    "\n",
    "tree_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_tree),\n",
    "    (\"reg\", tree)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"decision_tree\", tree_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661ce1b",
   "metadata": {},
   "source": [
    "### RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9dd625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RANDOM_FOREST METRICS ===\n",
      "R2: 0.8988\n",
      "MAE: 0.5503\n",
      "MSE: 1.7961\n",
      "RMSE: 1.3402\n",
      "EXPLAINED_VARIANCE: 0.8988\n",
      "CV R2 MEAN: 0.9278 | CV R2 STD: 0.0161\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(\n",
    "    transformers=[(\"imputer\", SimpleImputer(strategy=\"mean\"), FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    rf = TransformedTargetRegressor(regressor=rf, transformer=StandardScaler())\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_rf),\n",
    "    (\"reg\", rf)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"random_forest\", rf_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e077e76",
   "metadata": {},
   "source": [
    "### XGBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBOOST METRICS ===\n",
      "R2: 0.8906\n",
      "MAE: 0.6468\n",
      "MSE: 1.9189\n",
      "RMSE: 1.3852\n",
      "EXPLAINED_VARIANCE: 0.8907\n",
      "CV R2 MEAN: 0.9102 | CV R2 STD: 0.0172\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "preprocessor_xgb = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6,\n",
    "                   random_state=GLOBAL_SEED, n_jobs=-1, verbosity=0)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    xgb = TransformedTargetRegressor(regressor=xgb, transformer=StandardScaler())\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_xgb),\n",
    "    (\"reg\", xgb)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"xgboost\", xgb_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd1c86",
   "metadata": {},
   "source": [
    "### HISTGRADIENTBOOSTING REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e50be1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HIST_GRADIENT_BOOSTING METRICS ===\n",
      "R2: 0.8837\n",
      "MAE: 0.8123\n",
      "MSE: 2.9495\n",
      "RMSE: 1.7174\n",
      "EXPLAINED_VARIANCE: 0.8837\n",
      "CV R2 MEAN: 0.8919 | CV R2 STD: 0.0154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "# BUILD HISTGRADIENTBOOSTING PIPELINE\n",
    "def build_hgb_pipeline(use_target_scaling: bool = False) -> Pipeline:\n",
    "    base_hgb = HistGradientBoostingRegressor(\n",
    "        max_iter=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=GLOBAL_SEED\n",
    "    )\n",
    "\n",
    "    # WRAP IN MULTIOUTPUTREGRESSOR TO SUPPORT MULTI-TARGET REGRESSION\n",
    "    mo_hgb = MultiOutputRegressor(base_hgb)\n",
    "\n",
    "    # OPTIONALLY SCALE TARGETS USING TRANSFORMEDTARGETREGRESSOR\n",
    "    if use_target_scaling:\n",
    "        mo_hgb = TransformedTargetRegressor(\n",
    "            regressor=mo_hgb,\n",
    "            transformer=StandardScaler()\n",
    "        )\n",
    "\n",
    "    # BUILD PIPELINE WITH COLUMN TRANSFORMER TO PASS FEATURES THROUGH\n",
    "    pipe = Pipeline([\n",
    "        (\"dropper\", ColumnTransformer([(\"num\", \"passthrough\", FEATURE_COLS)], remainder=\"drop\")),\n",
    "        (\"reg\", mo_hgb)\n",
    "    ])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# BUILD AND EVALUATE HISTGRADIENTBOOSTING PIPELINE\n",
    "hgb_pipe = build_hgb_pipeline(use_target_scaling=USE_TARGET_SCALING)\n",
    "results.append(evaluate_sklearn_pipeline(\"hist_gradient_boosting\", hgb_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01fdd2",
   "metadata": {},
   "source": [
    "### LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eef31e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24\n",
      "[LightGBM] [Info] Number of data points in the train set: 1036, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "=== LIGHTGBM METRICS ===\n",
      "R2: 0.8924\n",
      "MAE: 0.7607\n",
      "MSE: 2.5348\n",
      "RMSE: 1.5921\n",
      "EXPLAINED_VARIANCE: 0.8925\n",
      "CV R2 MEAN: 0.8967 | CV R2 STD: 0.0168\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# SCALING FOR LGBM\n",
    "preprocessor_lgbm = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "base_lgbm = LGBMRegressor(n_estimators=100, learning_rate=0.1,\n",
    "                          max_depth=-1, random_state=GLOBAL_SEED, n_jobs=-1)\n",
    "\n",
    "# WRAP FOR MULTI-TARGET REGRESSION\n",
    "mo_lgbm = MultiOutputRegressor(base_lgbm)\n",
    "\n",
    "if USE_TARGET_SCALING:\n",
    "    mo_lgbm = TransformedTargetRegressor(regressor=mo_lgbm, transformer=StandardScaler())\n",
    "\n",
    "lgbm_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor_lgbm),\n",
    "    (\"reg\", mo_lgbm)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"lightgbm\", lgbm_pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9295a70a",
   "metadata": {},
   "source": [
    "### CATBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eee72ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATBOOST METRICS ===\n",
      "R2: 0.8847\n",
      "MAE: 0.7965\n",
      "MSE: 2.9470\n",
      "RMSE: 1.7167\n",
      "EXPLAINED_VARIANCE: 0.8848\n",
      "CV R2 MEAN: 0.8995 | CV R2 STD: 0.0156\n",
      "CATBOOST PIPELINE BUILT AND EVALUATED.\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# CATBOOST \n",
    "base_cat = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=GLOBAL_SEED,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# WRAP CATBOOST IN MULTIOUTPUTREGRESSOR TO HANDLE MULTI-TARGET REGRESSION\n",
    "mo_cat = MultiOutputRegressor(base_cat)\n",
    "\n",
    "# SCALE TARGETS USING TRANSFORMEDTARGETREGRESSOR\n",
    "if USE_TARGET_SCALING:\n",
    "    mo_cat = TransformedTargetRegressor(\n",
    "        regressor=mo_cat,\n",
    "        transformer=StandardScaler()\n",
    "    )\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"DROPPER\", ColumnTransformer([(\"NUM\", \"passthrough\", FEATURE_COLS)], remainder=\"drop\")),\n",
    "    (\"REG\", mo_cat)\n",
    "])\n",
    "\n",
    "results.append(evaluate_sklearn_pipeline(\"catboost\", cat_pipe))\n",
    "\n",
    "print(\"CATBOOST PIPELINE BUILT AND EVALUATED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2ebd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING REGRESSOR METRICS:\n",
      "R2: 0.8641\n",
      "MAE: 0.6315\n",
      "MSE: 2.9301\n",
      "RMSE: 1.7118\n",
      "EXPLAINED_VARIANCE: 0.8646\n",
      "CV R2 MEAN: 0.8923 | CV R2 STD: 0.0346\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING \n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, FEATURE_COLS)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# BASE MODELS \n",
    "def build_svr():\n",
    "    return (\"svr\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", SVR(C=3474.215, epsilon=0.0001141, gamma=\"scale\"))\n",
    "    ]))\n",
    "\n",
    "def build_rf():\n",
    "    return (\"rf\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "    ]))\n",
    "\n",
    "def build_xgb():\n",
    "    return (\"xgb\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "    ]))\n",
    "\n",
    "def build_lgbm():\n",
    "    return (\"lgbm\", Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=GLOBAL_SEED))\n",
    "    ]))\n",
    "\n",
    "estimators = [build_svr(), build_rf(), build_xgb(), build_lgbm()]\n",
    "\n",
    "# META LEARNER \n",
    "poly = PolynomialFeatures(degree=10, include_bias=False)\n",
    "lasso = Lasso(alpha=0.000794, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "meta_learner = Pipeline([\n",
    "    (\"poly\", poly),\n",
    "    (\"reg\", lasso)\n",
    "])\n",
    "\n",
    "# STACKING REGRESSOR \n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# WRAP IN MULTIOUTPUT REGRESSOR\n",
    "stacking_multioutput = MultiOutputRegressor(stacking, n_jobs=-1)\n",
    "\n",
    "# FINAL PIPELINE\n",
    "stacking_pipeline = Pipeline([\n",
    "    (\"stack\", stacking_multioutput)\n",
    "])\n",
    "\n",
    "# EVALUATE USING YOUR EVALUATION FUNCTION\n",
    "model_name = f\"STACKING REGRESSOR\"\n",
    "result = evaluate_sklearn_pipeline(model_name, stacking_pipeline)     # RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8972328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING REGRESSOR METRICS:\n",
      "R2: 0.8389\n",
      "MAE: 0.8875\n",
      "MSE: 3.3806\n",
      "RMSE: 1.8387\n",
      "EXPLAINED_VARIANCE: 0.8391\n",
      "CV R2 MEAN: 0.8659 | CV R2 STD: 0.0190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# COMBINED PREPROCESSING STEP INCLUDING POLY FEATURES\n",
    "preprocessor = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", RobustScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=5, include_bias=False)),\n",
    "])\n",
    "\n",
    "# FEATURE SELECTION (AFTER POLY)\n",
    "feature_selector = SelectKBest(score_func=f_regression, k=20)\n",
    "\n",
    "# BASE MODEL BUILDERS—NO INTERNAL PREPROCESSING\n",
    "def build_svr():\n",
    "    return (\"svr\", SVR(C=3474.215, epsilon=0.0001141, gamma=\"scale\"))\n",
    "\n",
    "def build_rf():\n",
    "    return (\"rf\", RandomForestRegressor(n_estimators=100, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "\n",
    "def build_xgb():\n",
    "    return (\"xgb\", XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=GLOBAL_SEED, n_jobs=-1))\n",
    "\n",
    "def build_lgbm():\n",
    "    return (\"lgbm\", LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=GLOBAL_SEED))\n",
    "\n",
    "estimators = [build_svr(), build_rf(), build_xgb(), build_lgbm()]\n",
    "\n",
    "# META LEARNER (LASSO)\n",
    "lasso = Lasso(alpha=0.000794, max_iter=10000, random_state=GLOBAL_SEED)\n",
    "\n",
    "# STACKING REGRESSOR WITH passthrough=True\n",
    "-stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=lasso,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED),\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# MAIN PIPELINE: PREPROCESSING + SELECTKBEST + STACKING\n",
    "stacking_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", feature_selector),\n",
    "    (\"stack\", stacking)\n",
    "])\n",
    "\n",
    "# MULTI-OUTPUT WRAPPER\n",
    "stacking_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "# EVALUATE\n",
    "model_name = \"STACKING REGRESSOR\"\n",
    "result = evaluate_sklearn_pipeline(model_name, stacking_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7d4b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=20...\n",
      "STACKING REGRESSOR K=20 METRICS:\n",
      "R2: 0.8389\n",
      "MAE: 0.8875\n",
      "MSE: 3.3806\n",
      "RMSE: 1.8387\n",
      "EXPLAINED_VARIANCE: 0.8391\n",
      "CV R2 MEAN: 0.8659 | CV R2 STD: 0.0190\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=25...\n",
      "STACKING REGRESSOR K=25 METRICS:\n",
      "R2: 0.8403\n",
      "MAE: 0.8701\n",
      "MSE: 3.3433\n",
      "RMSE: 1.8285\n",
      "EXPLAINED_VARIANCE: 0.8405\n",
      "CV R2 MEAN: 0.8677 | CV R2 STD: 0.0180\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=30...\n",
      "STACKING REGRESSOR K=30 METRICS:\n",
      "R2: 0.8524\n",
      "MAE: 0.7934\n",
      "MSE: 2.6200\n",
      "RMSE: 1.6186\n",
      "EXPLAINED_VARIANCE: 0.8524\n",
      "CV R2 MEAN: 0.8745 | CV R2 STD: 0.0217\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=35...\n",
      "STACKING REGRESSOR K=35 METRICS:\n",
      "R2: 0.8707\n",
      "MAE: 0.7354\n",
      "MSE: 2.5418\n",
      "RMSE: 1.5943\n",
      "EXPLAINED_VARIANCE: 0.8707\n",
      "CV R2 MEAN: 0.8860 | CV R2 STD: 0.0225\n",
      "\n",
      "RUNNING PIPELINE WITH SELECTKBEST K=40...\n",
      "STACKING REGRESSOR K=40 METRICS:\n",
      "R2: 0.8772\n",
      "MAE: 0.6847\n",
      "MSE: 2.1062\n",
      "RMSE: 1.4513\n",
      "EXPLAINED_VARIANCE: 0.8773\n",
      "CV R2 MEAN: 0.8915 | CV R2 STD: 0.0226\n",
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "    K |       R2 |      MAE |      MSE |     RMSE |  EXPL_VAR | CV R2 MEAN | CV R2 STD\n",
      "--------------------------------------------------------------------------------\n",
      "   20 |   0.8389 |   0.8875 |   3.3806 |   1.8387 |    0.8391 |     0.8659 |    0.0190\n",
      "   25 |   0.8403 |   0.8701 |   3.3433 |   1.8285 |    0.8405 |     0.8677 |    0.0180\n",
      "   30 |   0.8524 |   0.7934 |   2.6200 |   1.6186 |    0.8524 |     0.8745 |    0.0217\n",
      "   35 |   0.8707 |   0.7354 |   2.5418 |   1.5943 |    0.8707 |     0.8860 |    0.0225\n",
      "   40 |   0.8772 |   0.6847 |   2.1062 |   1.4513 |    0.8773 |     0.8915 |    0.0226\n"
     ]
    }
   ],
   "source": [
    "# RANGE OF K VALUES TO EVALUATE\n",
    "k_values = range(20, 45, 5)\n",
    "\n",
    "# LIST TO HOLD RESULTS FOR EACH K\n",
    "results_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nRUNNING PIPELINE WITH SELECTKBEST K={k}...\")\n",
    "\n",
    "    # UPDATE FEATURE SELECTOR WITH CURRENT K\n",
    "    feature_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "\n",
    "    # DEFINE THE PIPELINE FOR THIS K\n",
    "    stacking_pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", feature_selector),\n",
    "        (\"stack\", stacking)\n",
    "    ])\n",
    "\n",
    "    # WRAP WITH MULTIOUTPUTREGRESSOR\n",
    "    multioutput_pipeline = MultiOutputRegressor(stacking_pipeline, n_jobs=-1)\n",
    "\n",
    "    # EVALUATE PIPELINE (YOUR EXISTING EVALUATION FUNCTION)\n",
    "    model_name = f\"STACKING REGRESSOR K={k}\"\n",
    "    result = evaluate_sklearn_pipeline(model_name, multioutput_pipeline)\n",
    "\n",
    "    # STORE RESULTS WITH K FOR LATER COMPARISON\n",
    "    results_list.append((k, result))\n",
    "\n",
    "# AFTER ALL RUNS, PRINT SUMMARIZED COMPARISON\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "print(f\"{'K':>5} | {'R2':>8} | {'MAE':>8} | {'MSE':>8} | {'RMSE':>8} | {'EXPL_VAR':>9} | {'CV R2 MEAN':>10} | {'CV R2 STD':>9}\")\n",
    "print(\"-\" * 80)\n",
    "for k, res in results_list:\n",
    "    print(f\"{k:5} | {res['r2']:8.4f} | {res['mae']:8.4f} | {res['mse']:8.4f} | {res['rmse']:8.4f} | {res['explained_variance']:9.4f} | {res['cv_r2_mean']:10.4f} | {res['cv_r2_std']:9.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
