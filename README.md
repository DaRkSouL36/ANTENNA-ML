# MACHINE LEARNING REGRESSION MODEL FOR ANTENNA-PARAMETER PREDICTION

## PROJECT OVERVIEW

THIS PROJECT INVOLVES TRAINING MULTIPLE REGRESSION MODELS USING A DEEP LEARNING ARCHITECTURE TO PREDICT MULTIPLE TARGET PARAMETERS. THREE SEPARATE MODELS HAVE BEEN TRAINED, EACH PREDICTING DIFFERENT NUMBERS OF PARAMETERS AND EPOCHS:

- **Main{3]**: PREDICTING 3 PARAMETERS (EPOCHS = 1000)
- **Main{4]**: PREDICTING 4 PARAMETERS (EPOCHS = 5000)
- **Main{7]**: PREDICTING 7 PARAMETERS (EPOCHS = 1000)

EACH MODEL WAS TRAINED USING THE **KERAS** FRAMEWORK, AND THE DATA WAS PREPROCESSED WITH SCALING TO ENSURE OPTIMAL MODEL PERFORMANCE. THE MODELS USE DIFFERENT NEURAL NETWORK ARCHITECTURES WITH FULLY CONNECTED LAYERS AND RELU ACTIVATION, AND THEY ARE EVALUATED BASED ON VARIOUS METRICS, INCLUDING **R²**, **MEAN SQUARED ERROR (MSE)**, AND **MEAN ABSOLUTE ERROR (MAE)**.

---

## MODEL DETAILS

- **NEURAL NETWORK ARCHITECTURE:**
  - **INPUT LAYER:** CORRESPONDING TO THE NUMBER OF INPUT FEATURES.
  - **HIDDEN LAYERS:** MULTIPLE HIDDEN LAYERS WITH RELU ACTIVATION.
  - **OUTPUT LAYER:** OUTPUTS AS MANY NEURONS AS THERE ARE PARAMETERS BEING PREDICTED, WITH A **LINEAR ACTIVATION** FOR REGRESSION.
  
- **METRICS TRACKED DURING TRAINING:**
  - **MEAN SQUARED ERROR (MSE)**
  - **MEAN ABSOLUTE ERROR (MAE)**
  - **R² SCORE** (TO EVALUATE MODEL FIT)
  - **ACCURACY** (APPLICABLE MAINLY FOR CLASSIFICATION MODELS BUT CAN BE TRACKED FOR OVERALL PERFORMANCE)

- **OPTIMIZER:** ADAM OPTIMIZER FOR EFFICIENT TRAINING.
- **LOSS FUNCTION:** MEAN SQUARED ERROR (MSE) FOR REGRESSION TASKS.
  
  ---
## MACHINE LEARNING MODELS USED AND COMPARED

THE FOLLOWING MACHINE LEARNING MODELS WERE USED AND COMPARED DURING THE TRAINING OF THE MODELS:

1. **DEEP LEARNING MODEL (NEURAL NETWORKS)**  
   - FULLY CONNECTED LAYERS WITH RELU ACTIVATION AND LINEAR OUTPUT FOR REGRESSION.

2. **DUMMY REGRESSOR**  
   - BASELINE MODEL THAT PREDICTS THE MEAN OF THE TRAINING TARGETS.

3. **LINEAR REGRESSION MODEL**  
   - STANDARD LINEAR REGRESSION MODEL TO PREDICT PARAMETERS.

4. **RIDGE REGRESSION**  
   - LINEAR REGRESSION WITH L2 REGULARIZATION (RIDGE PENALTY).
  
5. **BAYESIAN RIDGE REGRESSION**  
   - REGRESSION USING BAYESIAN INFERENCE, WITH A REGULARIZED LINEAR MODEL.

6. **HISTOGRAM GRADIENT BOOSTING REGRESSOR**  
    - A FAST ENSEMBLE METHOD BASED ON GRADIENT BOOSTING, USING HISTOGRAMS FOR COMPUTATIONAL EFFICIENCY.

7. **LINEAR REGRESSION WITH POLYNOMIAL FEATURES**  
    - LINEAR REGRESSION MODEL AFTER APPLYING POLYNOMIAL FEATURE TRANSFORMATION.

8. **SUPPORT VECTOR REGRESSION (SVM) WITH RBF, LINEAR, AND POLYNOMIAL KERNELS**  
   - DIFFERENT KERNELS WERE USED TO EVALUATE THE PERFORMANCE OF SVM REGRESSORS.

9. **K-NEAREST NEIGHBORS (KNN) REGRESSOR**  
   - USED K=5 NEIGHBORS AND WEIGHTED THE PREDICTIONS BASED ON DISTANCE.

1. **DECISION TREE REGRESSOR**  
   - REGRESSION USING A DECISION TREE STRUCTURE.

1. **RANDOM FOREST REGRESSOR**  
   - AN ENSEMBLE METHOD USING MULTIPLE DECISION TREES.

---

## TRAINING THE MODEL

1. **LOAD DATA**  
   THE DATA SHOULD BE LOADED INTO PANDAS DATAFRAMES AND SCALED FOR TRAINING.

2. **MODEL ARCHITECTURE**  
   THREE MODELS ARE DEFINED, EACH CORRESPONDING TO THE NUMBER OF PARAMETERS BEING PREDICTED (3, 4, OR 7). THE ARCHITECTURE IS DEFINED IN THE SCRIPT AND COMPILED USING **ADAM** OPTIMIZER AND **MEAN SQUARED ERROR** LOSS FUNCTION.

3. **MODEL TRAINING**  
   THE MODELS ARE TRAINED WITH DIFFERENT EPOCHS:
   - **Main{3]**: **1000 EPOCHS**
   - **Main{4]**: **5000 EPOCHS**
   - **Main{7]**: **1000 EPOCHS**

   THE TRAINING PROGRESS, INCLUDING LOSS AND ACCURACY METRICS, IS PRINTED DURING EACH EPOCH.

4. **EVALUATING THE MODEL**  
   AFTER TRAINING, THE MODELS ARE EVALUATED USING VARIOUS METRICS SUCH AS **R²**, **MAE**, AND **MSE**.

---

## RESULTS

THE MODELS' PERFORMANCE CAN BE EVALUATED WITH THE FOLLOWING METRICS:

- **R² SCORE** - INDICATES HOW WELL THE MODEL FITS THE DATA.
- **MAE** - AVERAGE ABSOLUTE ERROR BETWEEN THE PREDICTED AND ACTUAL VALUES.
- **MSE** - AVERAGE OF THE SQUARED DIFFERENCES BETWEEN PREDICTED AND ACTUAL VALUES.

YOU CAN COMPARE HOW EACH MODEL PERFORMS FOR DIFFERENT SETS OF PARAMETERS BY LOOKING AT THE TRAINING AND VALIDATION PERFORMANCE PLOTS.

---

## CONCLUSION

THIS PROJECT DEMONSTRATES HOW TO APPLY DEEP LEARNING MODELS TO ANTENNA-PARAMETER REGRESSION TASKS. EACH OF THE MODELS TRAINED CAN PREDICT 3, 4, OR 7 PARAMETERS, AND THE EVALUATION RESULTS GIVE INSIGHTS INTO HOW WELL THE MODELS FIT THE DATA. THE FIGURES, METRICS, AND TRAINING RESULTS CAN GUIDE YOU IN SELECTING THE BEST MODEL FOR YOUR APPLICATION.

THE COMPARISON OF DIFFERENT MACHINE LEARNING MODELS HELPS TO UNDERSTAND THE STRENGTHS AND WEAKNESSES OF EACH MODEL FOR MULTI-PARAMETER PREDICTION TASKS.

---

## FUTURE WORK

1. **HYPERPARAMETER TUNING**  
   FURTHER IMPROVEMENT CAN BE MADE BY TUNING HYPERPARAMETERS SUCH AS THE NUMBER OF HIDDEN LAYERS, THE NUMBER OF NEURONS PER LAYER, THE LEARNING RATE, AND THE BATCH SIZE.

2. **DATA AUGMENTATION**  
   ADDITIONAL DATA PREPROCESSING, AUGMENTATION, AND FEATURE ENGINEERING CAN HELP IMPROVE MODEL PERFORMANCE.

3. **ADVANCED MODELS**  
   EXPLORING OTHER ADVANCED MODELS SUCH AS XGBoost MIGHT IMPROVE PREDICTION ACCURACY.

4. **ENHANCED TRAINING SETUPS**  
   TRAINING MODELS USING DIFFERENT DATA SPLITS, TIME SERIES DATA, OR CROSS-VALIDATION MAY PROVIDE ADDITIONAL INSIGHT AND IMPROVE GENERALIZATION.